{
    "title": "How dangerous is AI? - Quora",
    "tags": [
        "Risks of Artificial Intelligence",
        "Danger and Dangers",
        "Artificial Intelligence"
    ],
    "response": [
        {
            "author_info": {
                "name": "Yegor Tkachenko",
                "href": "/profile/Yegor-Tkachenko"
            },
            "answer_text": "AI can be deadly. First, there are two \u2018existential\u2019 threats. (a) AI can automate a lot of mundane jobs. This can put a lot of people out of work. While human society is exceptionally good at coming up with new jobs and things to do, it is far from clear that job creation process will out-speed job destruction process. Job destruction per se is not necessarily bad - the original purpose of all so-cherished technological progress has been to relieve humans of hard work - think of the wheel invention. However, job destruction can have bad consequences within this particular socio-politico-economic setup we are living in, where if you are out of job, and society has no need for you, you are going down. In particular, it is not impossible that we may end up in a situation where labor of a portion of population assisted by AI will be enough to provide all goods and services required by the whole population. Under current political regimes this could be catastrophic and lead to tensions between the few owners of the means of production and the non-owners. It is in response to these concerns that ideas such as basic income have been proposed and experimented with [Hawaii just became the first US state to pass a bill supporting basic income \u2014 here's the man behind it, Finland is testing universal basic income - and found it has had an unexpected side effect, Switzerland's voters reject basic income plan - BBC News]. The gravity of this concern and its growing popularity among younger people will also be a guiding force behind growing strength of socialist political forces in the Western societies. (b) AI can become sentient (conscious, possessing subjective perception and free will - or any other preferred definition), and then turn against people. This may seem farfetched, as the current state of machine learning technology does not offer a clear path to building a sentient machine from the ground up. In fact, people cannot even agree on what consciousness is, so it is quite hard to study it and engineer it. See, for example, these wonderful videos with Prof. David Chalmers.   However, while science might be quite clueless at this point on how to build up conscious brain (in part, because it has hard time finding instruments to objectively observe a person\u2019s subjective living inside his head), there has been definite progress in integrating existing animal or human brains (or their parts) with machines [\"Brain\" In A Dish Acts As Autopilot Living Computer, Brain Implant Gives Paralyzed Man Functional Control of Arm - Neuroscience News, Brain\u2013computer interface - Wikipedia]. In other words, it is not infeasible that at some point we will be able to integrate a living mouse with a military robot (one of these Military robot - Wikipedia), and then suddenly have this robot go out of control trying to kill people. What about integrating a mouse brain with an interface to the Internet? Can it learn to hack your bank account? We have yet to see. (To be fair, the brain-computer interface may not fit exactly the definition of \u2018artificial\u2019 intelligence, but it feels as an appropriate part of this answer). Second, there are two more threats, which are more mundane, but, I would argue, are more likely to be realized in the proximate future. (a) Current machine learning technology cannot yet create a conscious brain. However, it is already very good at extracting rules from a lot of data to guide one\u2019s behavior in order to try to achieve some objective mathematically optimally. For example, recent experiments by Google DeepMind have shown how computer can learn to play Atari games, using visual information from the game screen, sometimes achieving better game scores than a human [http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html?foxtrotcallback=true]. We also know that we have been quite good at creating autonomous driving robots [Waymo - Wikipedia]. Now, taking these technologies and putting them on top of a military robot is already absolutely feasible, with only minor technological challenges. There is, really, little fundamental difference to a computer between looking at Atari game pixel screen and picking actions to optimally shoot down space ships, and looking at a pixel screen of a real-world camera and picking actions to optimally shoot down people. With only a nascent international legal framework to control the use of such autonomous machines, we may soon end up in a world where machines programmed to kill with mathematical precision will outmatch both in deadliness and numbers ordinary human soldiers. What if such technology is developed to fruition by a country or group of people who do not have respect for human life and freedom? It remains to be seen. (b) Finally, even if AI does not gain sentience, and is programmed only with good intentions in mind, there still remains a possibility of an error. Whether it is an error during the control of a nuclear power plant, or your new shiny Tesla car, or a missile launch, or a stock exchange, or an automated dispute resolution system, the errors can be far more grave in consequences than any naughtiness intentionally programmed in. Whereas previous issues are more societal in scope, and are up for higher powers to adjudicate, this issue of errors is something AI developers bear the most immediate responsibility for, and have to take great care to plan for and prevent.  There may be other threats that AI poses to humans, but I feel the above four are the most critical ones and deserve careful thinking by politicians, business leaders, and engineers alike. However, does all this risk mean we should restrict the development of AI? I would argue against it, as possible benefits (freedom from hard undesirable labor, discovery of new cures to diseases, safer driving, etc.) so far seem to outweigh the cost, even if the society may have to fundamentally change its structure and regulation to accommodate automation. Cautious optimism in the face of uncertainty seems to be the right way to go. ",
            "date": "Answered July 26, 2017",
            "views": "179",
            "upvotes": " View 47 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Glenn Yurchenko",
                    "user_href": "/profile/Glenn-Yurchenko"
                },
                {
                    "user_id": "Rajiv Kumar",
                    "user_href": "/profile/Rajiv-Kumar-489"
                },
                {
                    "user_id": "Abhishek Verma",
                    "user_href": "/profile/Abhishek-Verma-625"
                },
                {
                    "user_id": "Jo\u00e3o Vitor",
                    "user_href": "/profile/Jo\u00e3o-Vitor-43"
                },
                {
                    "user_id": "Ahmadzai",
                    "user_href": "/profile/Ahmadzai-2"
                },
                {
                    "user_id": "Kumar K",
                    "user_href": "/profile/Kumar-K-546"
                },
                {
                    "user_id": "Pranav Kulkarni",
                    "user_href": "/profile/Pranav-Kulkarni-166"
                },
                {
                    "user_id": "Anurag Kashyap",
                    "user_href": "/profile/Anurag-Kashyap-170"
                },
                {
                    "user_id": "Deji Atoyebi",
                    "user_href": "/profile/Deji-Atoyebi"
                },
                {
                    "user_id": "Arju Kundu",
                    "user_href": "/profile/Arju-Kundu"
                }
            ]
        },
        {
            "author_info": {
                "name": "Ken Hutchison",
                "href": "/profile/Ken-Hutchison"
            },
            "answer_text": "Oh, goodness. AI is not dangerous. I'd love to yell this from a rooftop. Once the functions and algorithms are designed it's hard to see why things do what they do, but there are many such black boxes in the world, the human body being one of them. This is because of interaction effects and higher order things. Humans have trouble thinking very nonlinearly and in networks because we are not blessed with the RAM to do so. Until we can either create human level AI, which is likely very very very far away, or use biology in tandem [augmenting a test tube brain with silicon, which amounts to the above] then our reasons to fear AI are people related. Primarily, programmer related. Badly designed AI can do dangerous things, like malfunction in a pacemaker or drive your car into a wall, or make horrible financial trades and ruin money markets. Well designed malicious AI can be malicious, and AI will displace many human jobs. During the Renaissance machines were feared, try arguing that continuing was a bad choice. AI itself is not the problem! Please, dispel this myth that AI skynet things are possible or predictable with our current state of the technology, they aren't and we don't know if they will be. The human species is clever, but we barely understand the nature of the grey goop in our heads, it's not likely we'll be able to replicate it. We live in a world where we still have beliefs in very mystical things, and there are as many opinions about consciousness as there are people, probably. Intelligence and sentience is certainly a replicable phenomena, but if we do, we will give much thought to how to restrict it. Suppose we could breed a tiger the size of a skyscraper, this might one day be possible [even though it probably isn\u2019t due to volume limitations,] that would sure be dangerous too. Humans are capable of many dangerous things. AI is a thing we know we can certainly limit and control implicitly by its very nature. Code isn\u2019t magic, it just looks that way. Our responsibility as a human race, is to not fear the unpredictable, but rather to embrace it. We won\u2019t accidentally create a sentient AI with malicious capability if we understand consciousness well enough to construct one. We have people wandering houses looking for paranormal events and atomic bombs sitting in shelters controlled by floppy disks [seriously] so you probably shouldn\u2019t be too worried about the infant technologies that like to curate news stories for you or recommend products to buy. Fantastic Question, Carry On! ",
            "date": "Answered September 24, 2016",
            "views": "14",
            "upvotes": " View 4 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Ruby Bean",
                    "user_href": "/profile/Ruby-Bean"
                },
                {
                    "user_id": "Kaarel Allemann",
                    "user_href": "/profile/Kaarel-Allemann"
                },
                {
                    "user_id": "Soundar Kumara",
                    "user_href": "/profile/Soundar-Kumara"
                },
                {
                    "user_id": "Yasemin Timar",
                    "user_href": "/profile/Yasemin-Timar"
                }
            ]
        }
    ]
}