{
    "title": "Why is the p-value criticized? And what are possible alternatives? - Quora",
    "tags": [
        "P-value (statistics)"
    ],
    "response": [
        {
            "author_info": {
                "name": "Tiberiu Tesileanu",
                "href": "/profile/Tiberiu-Tesileanu"
            },
            "answer_text": "Here\u2019s the short story: the -value is a measure that\u2019s used in null-hypothesis significance testing (NHST) that has several weaknesses: it ignores prior knowledge; it encourages the use of cooking-book-style statistical recipes; it answers the wrong question; it wrongly emphasizes straw-man hypotheses; it\u2019s a form of \u201cproof by approximate contradiction\u201d (which is statistically unsound); and more. What can we do instead? One simple alternative that addresses many of the issues with -values is to estimate parameters instead of trying to accept/reject discrete hypotheses. More importantly, though, it\u2019s best to keep in mind that data is messy and there is no statistical method that will deliver certainty.  Now, the detailed story is long. But if you\u2019re dedicated enough, here it is below. I\u2019ll start by briefly explaining what the -value is, how and why it\u2019s used, and some common misconceptions. Then I\u2019ll write about the failings of this approach. And I\u2019ll end with some discussion of alternatives. What is the -value? The -value is the probability of obtaining a measurement result that is at least as extreme as what was actually measured, under the assumption that the \u201cnull\u201d hypothesis  is true.  What is the \u201cnull\u201d hypothesis, you ask? In Fisher\u2019s original formulation, the null hypothesis was supposed to be the default, accepted idea; the thing you\u2019d believe if the data didn\u2019t change your mind. There\u2019s good reason for this (more below), but this concept is often either ignored or applied very loosely nowadays. Instead, people often (mistakenly) claim that the null hypothesis has to be the assumption that nothing happens, or that there\u2019s no difference between two groups, or that a certain parameter is zero (also known as the \u201cnil\u201d hypothesis). None of these restrictions are either necessary or sufficient for a proper application of -values. Null hypothesis significance testing (NHST) The -value is typically used in a hypothesis testing framework, in which obtaining a low -value \u2014 that is, roughly speaking, observing a measurement result that is unlikely given the null \u2014 allows us to \u201creject\u201d the null hypothesis. The logic has a misleading simplicity to it: if I believe something that says X will probably happen, and Y happens instead, then chances are what I believed was wrong. I\u2019ll explain below some ways in which this logic can fail.  (xckd: Null Hypothesis) The alternative hypothesis It\u2019s common to learn about and even use -values regularly, all while ignoring an essential concept that is at work behind the scenes: the alternative hypothesis. Confusion abounds about what this is. Some say it has to be the negation of the null hypothesis, but this is unnecessary and even counterproductive. Instead, think of the alternative as simply a second hypothesis, , that we wish to contrast with the null, and that we think might be true instead of the null. So, where does the alternative hypothesis lurk? The answer lies in a seemingly innocuous phrase in the definition of the -value: a result that is at least as extreme as what was measured Did you catch that? What\u2019s an \u201cextreme\u201d result? Who decides if a result is extreme or not? By now, you can probably guess the answer: it\u2019s the alternative hypothesis. What we call \u201cextreme\u201d actually means \u201cmore in line with the alternative than with the null\u201d. Let\u2019s look at an example. Suppose we\u2019re testing whether a new COVID-19 treatment works. We would split people into two groups: a control group that doesn\u2019t receive the treatment, and a treatment group that does. We would measure the recovery times for people in the two groups. Now we\u2019re facing our first issue: the data is two-dimensional! What is \u201cextreme\u201d in two dimensions?  So here\u2019s the first place where the alternative hypothesis comes in: we\u2019re going to define a so-called \u201ctest statistic\u201d, a single number that summarizes the data that we have in a way that allows us to test the null hypothesis. If my alternative hypothesis is that the drug shortens recovery time, then a reasonable idea for a test statistic would be to take the difference between the recovery time in the treatment group and the recovery time in the control group. This is a number, and if we assume that the null hypothesis is \u201cthe treatment does nothing\u201d, we would expect this number to be close to zero. What is \u201cmore extreme\u201d? Well, the alternative is that the treatment works, which means that the difference (recovery_time_treatment - recovery_time_control) would be negative; thus, \u201cmore extreme\u201d means \u201cmore negative\u201d. Notice how if this wasn\u2019t a treatment but a risk factor for COVID-19, we would have instead said that \u201cmore positive\u201d is more extreme. But there\u2019s more! Imagine that we have a treatment that doesn\u2019t shorten the average recovery time, but it reduces its variability, a bit like the red vs. blue distributions below:  If we wanted to test for the alternative \u201cvariability in recovery times is smaller in the treatment group\u201d, then we wouldn\u2019t take the different between mean recovery times. Instead, we might take a ratio of variances as a test statistic. The point here is that the alternative hypothesis is instrumental in deciding (a) how to collapse our data to a single number; and (b) what we mean by \u201cextreme\u201d values of this number. That said, it\u2019s worth keeping in mind that the null and the alternative hypotheses enter asymmetrically in this formulation of NHST (due to R. Fisher): the null must be a precise, mathematical statement that can be used to calculate probabilities; while the alternative can be vague and need only help us decide on a summary statistic to use and what to count as \u201cmore extreme\u201d. (This is different from the Neyman-Pearson approach to significance testing, where both the null and the alternative are taken seriously. That methodology is used far less often in practice, though, so I will focus on Fisher\u2019s approach for now.)  This odd relation between NHST and the alternative hypothesis is dangerous because people often either forget the importance of the alternative \u2014 and then get confused about such questions as \u201cshould I use a one-tailed or a two-tailed test\u201d \u2014 or conversely, believe that rejecting the null is paramount to accepting the alternative. In fact, in some fields scientists often start with an idea that explains the data, call this the alternative hypothesis, then formulate a null hypothesis that contradicts it, and then devise a test. When they find a small -value in that test, they erroneously conclude that they have found evidence for their initial hypothesis, when in fact they at most found evidence for the entire class of hypotheses that would have led to the same definition of \u201cmore extreme\u201d.  So what\u2019s great about null-hypothesis significance testing? Why do people use it? Coverage guarantees There is a very nice, mathematical reason for which methods such as NHST are preferred by many statisticians: the coverage guarantees.  (xkcd: Coverage) Ok, different kind of coverage. You see, null-hypothesis significance testing is basically an algorithm. It tells you that, after you make a measurement, you need to calculate the -value associated with the null hypothesis , and then declare that you reject  if  is small enough, say, , and otherwise do nothing (or \u201cfail to reject\u201d). The value  here is called the significance level. Now, the magical thing is that, once you set the significance level, if you keep applying this same algorithm to every decision you\u2019re making, you\u2019re guaranteed to wrongly reject the null just a fraction  of the time. This is amazing, right? Using a very simple algorithm, and choosing a small-enough , we can make sure to put a comfortable upper bound on how often we\u2019re wrong! Or\u2026 can we? Doesn\u2019t that sound a bit too good to be true? Well, it does, but it is true, in a sense. It\u2019s a mathematical theorem: if the assumptions are obeyed, then the coverage guarantee holds. Now, the devil is in those assumptions, but for now, let\u2019s take a moment to appreciate what we have before we go on and ruin it. Simplicity One sociological advantage of the -value is that they\u2019re easy to use. Tests for many different kinds of null hypotheses have already been devised and implemented in widely available statistical software, and most research problems can be expressed in a form that is at least superficially compatible with one of those tests. That means that the statistical analysis of your data can be a sort of mindless thing: search for a good-enough test, run it, and check whether the -value is below or above the significance threshold. Is it ideal to use an off-the-shelf method like that? No. But it sure is convenient. Think of it as eating at McDonalds: it\u2019s not earth-shattering, but it\u2019s fast, cheap, always there, and not entirely unsatisfying, either.  But what exactly is wrong with -values? Sociological issues Much like eating at McDonalds, there\u2019s nothing wrong with using -values sparingly and in the proper contexts. So let\u2019s start with the ways in which people misuse -values.  (xkcd: Clickbait-Corrected p-Value) One such issue is known as p-hacking. You see, the NHST algorithm is clear: reject null if , do not reject otherwise. But people get attached to ideas. So, if the -value ends up being just above the significance level , we might start to wonder: did we make a mistake? should we have used a higher significance level? should we have analyzed the data in a different way? And so on.  (xkcd: P-Values) It\u2019s important to understand that there\u2019s nothing fundamentally wrong with this. Science isn\u2019t straightforward. Sometimes we do make a mistake. Sometimes we do use the wrong method. Sometimes there\u2019s just bad luck and our data fail to see an effect that is there. It\u2019s good for the scientist to keep all of these things in mind. But the problem is that if she acts on them, the coverage guarantees that we had are lost. And of course, this can also be used maliciously: a researcher can intentionally remove part of the data, or change the analysis or the significance level used in order to obtain a statistically significant result. A related issue is that of multiple comparisons. From the very definition of the -value we see that, if we apply the null-hypothesis significance testing algorithm to, say, 100 different problems, we will reject the null hypothesis approximately  times even if all the 100 null hypotheses were true.  (xkcd: Significant) So, another way to abuse NHST is to try many similar experiments, and cling on to the one that does yield a statistically significant result. Again, it should be emphasized that there\u2019s nothing wrong with doing this in an exploratory phase: we often have only a vague idea of what we\u2019re looking for before running an experiment, and only after seeing the data can we formulate a more specific hypothesis for what\u2019s going on. The problem occurs when we treat the results of such multiple comparisons as the final result (without appropriate corrections for the multiple comparisons) instead of seeing them as an exploratory step that needs further research to be substantiated. In this case as well, the issue can be either unintentional or fraudulent \u2014 with people intentionally reporting only one of hundreds of experiments that happened to \u201creach statistical significance\u201d. And then there is of course the simplicity issue which we listed as a reason people use the method. The flip side of that is that it gives people a very simple number that is very tempting to latch on to. We run an experiment and we get , and all of a sudden we think we\u2019ve \u201cshown\u201d some fact \u2014 when reality is much murkier than that. Prior knowledge So let\u2019s now delve a bit into the mathematical details of what the -value is and isn\u2019t telling us. After all, if the -value was indeed as magical as people thought, it wouldn\u2019t be a problem that it\u2019s used so often. One of the most glaring issues with NHST is that it completely ignores prior knowledge. Suppose we were testing the null hypothesis that \u201cnothing moves faster than the speed of light in vacuum, 300,000 km/s\u201d, and after we run the experiment, we find something that\u2019s moving at 300,001 km/s. It was a very sensitive experiment, so we calculate the -value and we find . Should we reject the null hypothesis? Well, something very similar to this actually happened some years ago (Faster-than-light neutrino anomaly). And the, perhaps surprising, answer is that we probably should not reject the null \u2014 and indeed most physicists dismissed the finding. Say what?!  (xkcd: Neutrinos) You see, this isn\u2019t the first measurement that tested . There are literally thousands of experiments that confirm both this specific hypothesis, and the more general theoretical framework from which it was obtained (the theory of relativity). Shouldn\u2019t all this prior knowledge account for something? Of course it should! It\u2019s surely unlikely to see particles moving so fast if the theory of relativity is true. But it\u2019s even more unlikely that every other experiment was wrong! Extraordinary claims require extraordinary evidence.  (xkcd: Frequentists vs. Bayesians) In the neutrino case, it turns out the (erroneous) result was due to a loose cable. The theory of relativity is fine. Coverage broken promises So, remember the coverage guarantees that sounded a bit too good to be true? Well\u2026 here\u2019s why they aren\u2019t as useful as they might seem. When you reject a hypothesis depending on whether , you\u2019re ensuring that you won\u2019t be wrong more than a fraction  of the time \u2014 of the time when the null hypothesis is true! You read that right. To obtain the coverage guarantee you have to assume that the null hypothesis is true. This should be clear since that\u2019s the assumption that we\u2019re making to define the -value. You would be entitled to think: \u201cwell, if we know that the null is true, why are we testing it?!\u201d. And if we did know for absolute sure, there would indeed be no point. But we\u2019re never 100% sure of anything, right? The coverage guarantees offered by the -value are most useful when we\u2019re testing a well-established idea (e.g., nothing travels faster than the speed of light in vacuum) that nevertheless could conceivably fail to be true. (Of course, then we have to make damn sure we don\u2019t have any loose cables in our experiment!) This is why Fisher insisted that the null should be our default position, the thing that we already believe and would keep believing if the data didn\u2019t convince us otherwise: we would implicitly be making decisions in a setting in which the null was likely true, thus making the -value a more meaningful quantity to look at. A different kind of issue with the coverage guarantees is related to the fact that they only hold if the null distribution \u2014 that is, the distribution of the test statistic that is implied by the null hypothesis \u2014 is compatible with reality. But this is almost never the case. For instance, coming back to our COVID-19 example, it would be standard to assume that the distribution of the difference in recovery times between the treatment and the control groups is a Gaussian. However, since biology is complicated, this is almost certainly not the case. Now, if we run our experiment and find a statistically significant answer, it might mean that the treatment works \u2014 or it might mean that we had a large-enough sample to detect that the distribution is not normal. The forgotten alternative hypothesis When trying to decide between two hypotheses, the null and the alternative, you can make two kinds of errors. A Type I error is when you reject the null hypothesis even though it\u2019s true. A Type II error is when you fail to reject it even though it\u2019s false. The coverage guarantee for NHST tells you that you can\u2019t have more than  rate of Type I errors; but it says nothing at all about Type II. Unfortunately, significance testing is often used in cases where Type II errors are common. Part of the reason is the approach I described above: often scientists are taught to set the alternative hypothesis as the hypothesis that they most believe is true, and have the null be a straw man that they proceed to reject using the data. In this case, Type I errors are less likely \u2014 after all, we built the null so that it\u2019s unlikely to be true! \u2014 and Type II errors should be considered. But the typical methods only report a -value and say nothing at all about Type II errors.  (xkcd: Error Types) How can you estimate Type II errors? By taking the alternative hypothesis seriously. The rate of Type II errors is essentially the -value calculated for the alternative. Of course, to be able to calculate that, we need to have a precise statement, so that we can calculate the distribution of the test statistic under the alternative hypothesis. We can then take the ratio of the -values calculated under the two hypotheses to help us decide whether or not to reject the null in favor of the alternative. This more symmetric approach to significance testing was advocated by Neyman and Pearson and generally has better properties than the Fisherian version. However, it is harder to use (since we need to specify a precise alternative), so it\u2019s rarely used in practice. Continuous parameters, effect size, and actual significance Null-hypothesis significance testing handles a discrete set of hypotheses: we\u2019re trying to reject one hypothesis, perhaps in (weak) favor of another. In doing so, it suffers from a lot of poorly chosen language. For instance, when the data is deemed strong enough to reject a null hypothesis (e.g., ), it is said that the result is \u201cstatistically significant\u201d. Unfortunately, this is often shortened to \u201csignificant\u201d, which takes on a very different meaning. And this leads us to the concept of effect size. You see, typically there isn\u2019t a sharp distinction between \u201cno effect\u201d and \u201csome effect\u201d. Instead, there is a continuum of possibilities, parameterized by an effect size. Think about the speed of light. At one point, people thought light travelled instantaneously. Then we found out that it doesn\u2019t \u2014 but not only that, we measured the speed at which it does travel. So this was never really a binary question (\u201cis the speed of light finite\u201d), but rather an estimation question (\u201chow fast does light travel\u201d). A different way to think of this is that there is not one alternative, but infinitely many: one for each value of the speed of light. It should be clear that NHST is not particularly well-suited in such cases \u2014 after all, imagine if we measured the speed of light by binary search. Project #1: is it larger than 1m/s? Yes. Project #2: is it larger than 2m/s? Yes. Project #3: etc. Does that seem reasonable?\u2026 Now, the nice thing about the size of an effect is that it can have meaning beyond just the statistical question of \u201cis it different from zero\u201d. For instance, if a study looked at the diets of hundreds of thousands of people, it might conclude that drinking a glass of wine every day increases their life expectancy by 1 day, . (Such studies do in fact exist, but I\u2019m making up more extreme numbers to make a point.) This is a statistically significant result \u2014 but it\u2019s not actually significant in the usual sense of the word, is it? It\u2019s not like you\u2019d be convinced to drink wine if you didn\u2019t before just to gain that extra one day. Statistical significance in this case does not translate to real-life significance; instead it\u2019s more of a measure of how large the sample size was in the study.  (xkcd: Boyfriend) Significance testing answers the wrong question When faced with interpreting the -value, people are often tempted to think that, e.g.,  means that with 99% probability, the null hypothesis is wrong. This is not true, for a few different reasons (see below). However, this fact underlines an important point: most of the times the actual question we\u2019re trying to ask is very different from the question that significance testing answers. We don\u2019t really care about the probability of the data given the hypothesis; but rather we\u2019re interested in the converse, how much to believe the hypothesis given the data. Bayes\u2019 theorem,  allows us to connect the two, and it shows some of the ways in which these quantities can differ. Prior probabilities  (xkcd: Seashell) The -value is, roughly, the probability of the data given the hypothesis. The term in Bayes\u2019 theorem that multiplies this probability, , is called the prior probability of the hypothesis. It formalizes the notion that I mentioned earlier that if we have reasons to believe that something is unlikely, we should need stronger evidence to convince ourselves that it happened (\u201cextraordinary claims require extraordinary evidence\u201d). How do prior probabilities affect our understanding of data? Here\u2019s a standard example. Suppose there\u2019s a rare disease that affects about 1 in 1000 people, and there\u2019s a test that is 99% accurate, in the sense that 99 out of 100 people who are sick are identified as such, while one sick person is missed by the test; and also 99 out of 100 people who are healthy are identified as such, while one is incorrectly diagnosed as sick. Now suppose you take the test and it comes out positive. How likely do you think it is that you are actually sick? It\u2019s very tempting to say that the probability is high. Maybe you wouldn\u2019t say 99%, but 90% certainly comes to mind. Yet, that\u2019s completely off \u2014 the actual probability is lower than 10%! How?! Well, imagine that 100,000 randomly chosen people get tested. You would expect about 100 of them to be sick, since the incidence of the disease is 1 in 1000. The test is 99% accurate, so out of the 100 sick ones, the test will flag 99 as sick and 1 will (wrongly) be identified as healthy. But here\u2019s the kicker: out of the remaining 99,900, who are all healthy, the test will mistakenly claim that 999 are sick! That means that we have almost 1100 sick diagnoses out of which only about 100 are correct! That\u2019s less than 10%! By ignoring prior knowledge, significance testing implicitly assumes that the hypotheses that are being contrasted are approximately equally likely \u2014 and that\u2019s often not true. That\u2019s especially the case when the null is something like \u201cthe effect is zero\u201d and the alternative is \u201cthe effect is literally anything else\u201d \u2014 how likely do you think it is for an effect to be precisely zero and not some small number, say, ? Proof by approximate contradiction doesn\u2019t work You might be familiar with proof by contradiction from school. It works like this: you start with an assumption that is the logical negation of the statement you\u2019re trying to prove. You then work out its consequences, and find that you run into a logical contradiction. You thus conclude that the initial assumption must have been wrong, and it follows that the statement you were trying to prove is true. This is a perfectly valid logical argument.  (xkcd: Principle of Explosion) Null-hypothesis significance testing often has a very similar flavor. In order to find evidence for a hypothesis, , one generates a \u201cnull\u201d hypothesis  that\u2019s the negation of the first. Then one runs an experiment and finds a result that is unlikely given the null. One then concludes that the null is likely false. This is almost identical to proof by contradiction, right, so it must work? Except for one key flaw: the contradiction is not exact \u2014 it\u2019s merely unlikely. And that makes all the difference. Paraphrasing an example from an old-ish paper (The earth is round (p<.05)), imagine someone trying to investigate the hypothesis that Barack Obama is not an American citizen. He reasons like this: let\u2019s make the \u201cnull\u201d hypothesis that Obama is a citizen. Our intrepid scientist researches Obama and finds that he was president. He calculates the -value for this observation and finds  \u2014 after all, there are only 5 living presidents of the US in a population of over 300 million. So he feels justified to reject the null hypothesis, and declare that Obama is not a citizen. This doesn\u2019t make any sense, does it? The reason is that, while it\u2019s unlikely for a citizen to have been president, it\u2019s even more unlikely for a non-citizen to have been one \u2014 in fact, the latter is not allowed. This goes to show one of the ways in which proof by approximate contradiction doesn\u2019t work: unlikely things happen all the time. Only impossible ones don\u2019t. Focus on rejection Another harmful side effect of the obsession with -values is the pervasive belief that one can only reject a hypothesis, but never prove it.  (xkcd: Popper) This is wrong not because you can prove hypotheses \u2014 you can\u2019t prove anything with 100% certainty \u2014 but because generically it\u2019s just as hard to reject them. This should be obvious from a logical standpoint: rejecting a hypothesis is the same as accepting its negation, so if you can do one, you can clearly do the other. It should also be clear from intuition: rejecting the hypothesis \u201cthere are bacteria in this fridge\u201d is no easier than accepting the hypothesis \u201cthere no apples in this fridge\u201d:  Indeed, one of the ideas hammered into science students\u2019 heads in introductory statistics courses is that failing to reject the null hypothesis doesn\u2019t provide evidence in its favor. But this isn\u2019t always true, and it\u2019s in fact contrary to how the method is applied: whether or not we make this explicit in our papers, we take repeated failures to reject an idea as evidence in favor of it. For instance, we believe the theory of evolution because we\u2019ve tried many times to find evidence against it, and failed. Clearly, failure to reject does imply evidence in the hypothesis\u2019 favor, at least sometimes. The problem, of course, is that many times failure to reject the null hypothesis does not provide strong evidence in its favor \u2014 and the -value gives us no clue which case is which.  Alternatives So, let\u2019s pretend that I convinced you that significance testing isn\u2019t so great\u2026 What can we do instead? I want to first reiterate this: the most important thing to do is to stop thinking that statistics can deliver certainty, and to avoid assigning too much importance to any single measure of \u201cstatistical significance\u201d. If we use the -value as one of several tools to help us understand our data, and we keep in mind that, no matter how careful we are, only further research can solidify our findings, then we\u2019re on a good path.Next: use parameter estimation instead of significance testing (but also remember the point above: no method will give you perfection!). For virtually every question, the size of the effect is important. We don\u2019t just care whether a gene contributes to breast cancer; we care how much. Moreover, we can use the confidence interval on the parameter to perform the same task as the -value \u2014 e.g., if the , then 0 is outside the 95% confidence interval. Parameter estimation answers both the significance question and a more useful, quantitative, question, it employs precise alternatives rather than the vague ones implied by rejecting the null, and \u2014 if done in the form of Bayesian credible intervals \u2014 can easily take advantage of prior knowledge.Remember: statistical tests can\u2019t give us certainty. Nothing beats replication. If an effect is true, we should see it in many different ways, in many different experiments, from many different groups. No amount of statistics can compete with that.Tailor statistical methods to the problem at hand instead of using cookbook-style recipes for statistical analysis. People keep using -values and forcing every research question into the NHST framework because that\u2019s the hammer they have, and so they want everything to be a nail. Not surprisingly, you can get better results by using better tools.Use Bayesian stats. In most scientific applications, we want statistics to answer the question \u201chow much should I believe in this idea after seeing the data\u201d. The Bayesian approach is a mathematically consistent framework for performing belief updates (in some sense, the only one: Cox's theorem - Wikipedia). There are numerous advantages to this method: it automatically includes base-rate information, it forces you to grapple with the implications of prior knowledge, it makes it relatively straightforward to incorporate arbitrary details of your models. This often comes with a steep computational price, but it\u2019s becoming less of an issue with better algorithms, better software, and better hardware.If you do use binary tests, formulate both a precise null and a precise alternative hypothesis, and use something like Neyman-Pearson\u2019s likelihood ratio. Or use Bayes. The fact that it\u2019s hard to formulate a precise alternative is no excuse \u2014 without an alternative it is literally impossible to reject anything.Use more than one of these alternatives. No one method does it all! If you\u2019ve read through and not just scrolled down here, congratulations \u2014 I\u2019m impressed! Here\u2019s a short summary: -values answer a very specific question (how likely is this data [or something \u201cmore extreme\u201d] to have been seen if the null hypothesis is true), and does so very well (i.e., we have coverage guarantees telling us how often we\u2019ll be wrong, in a certain sense). Unfortunately, this is typically not the question we want answered, and it\u2019s easy to get confused; this ignores prior knowledge; it misleadingly downplays the importance of the alternative hypothesis; and it encourages the mindless application of standard statistical techniques when more nuanced approaches would work better. We can instead focus on parameter estimation, which can be done either in a frequentist or Bayesian setting, and addresses all of the downsides of the -value without missing much, if anything. ",
            "date": "Answered September 25, 2020",
            "views": "14",
            "upvotes": " View 25 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Anupam Nayak",
                    "user_href": "/profile/Anupam-Nayak-12"
                },
                {
                    "user_id": "Rajesh S Rajagopalan",
                    "user_href": "/profile/Rajesh-S-Rajagopalan"
                },
                {
                    "user_id": "Raghavendra S. Mupparthy",
                    "user_href": "/profile/Raghavendra-S-Mupparthy"
                },
                {
                    "user_id": "Murali Krishna",
                    "user_href": "/profile/Murali-Krishna-4896"
                },
                {
                    "user_id": "Sawood Bokhari",
                    "user_href": "/profile/Sawood-Bokhari"
                },
                {
                    "user_id": "Manav Midha",
                    "user_href": "/profile/Manav-Midha-2"
                },
                {
                    "user_id": "Atsushi Takayama",
                    "user_href": "/profile/\u9ad8\u5c71-\u6e29-Atsushi-Takayama"
                },
                {
                    "user_id": "Joe Jacobs",
                    "user_href": "/profile/Joe-Jacobs-15"
                },
                {
                    "user_id": "Kenneth Theodore",
                    "user_href": "/profile/Kenneth-Theodore-4"
                },
                {
                    "user_id": "Roberto De Abreu",
                    "user_href": "/profile/Roberto-De-Abreu-1"
                }
            ]
        },
        {
            "author_info": {
                "name": "Aaron Brown",
                "href": "/profile/Aaron-Brown-165"
            },
            "answer_text": "There are two main objections to p-value, neither of which are recent. While nearly all statisticians acknowledge both objections, the recent event of the last few years is that a critical mass of professionals seems to have accepted that the objections are serious enough to discard p-value, or to significantly amend the way it is applied. Suppose you test a new drug on a population of patients, and compare outcomes to a randomly assigned matched control sample treated with existing methods. You want to know the probability that the new drug is better than existing methods. A statistician can\u2019t tell you that. What she can tell you is the probability that you would have assigned people to the treatment and control groups in such a manner as to get the results you got if the drug makes no difference at all. This is the p-value. Of course, this isn\u2019t what you want to know. It\u2019s a statement about the randomness you introduced in assigning patients, not about the utility of the new drug. Moreover, it\u2019s not reasonable to think any drug has absolutely no effect of any type, especially any drug worth testing clinically on humans. So you have the probability of something you\u2019re not interested in, under an assumption you know to be false. The main alternative is a Bayesian confidence interval. This gives you a range of plausible effectiveness for the new drug, given all the information. The objection to this is that it includes subjective prior information about the drug, two researchers looking at the same data will have different intervals depending on their prior beliefs. Depending on how you feel about the relative value of objectivity versus accuracy, there are ways to address this issue. The second objection is that p-values in practice are not computed according to the rubric that makes them objective. Researchers typically look at the data, then decide what significance it has. They often fail to adjust for all the hypotheses considered, and treat their conclusion as if it were pre-specified and the only statement tested. For these and other reasons, it is not hard for a determined researcher to find statistical significance at conventional levels in any data. This means that the only clear advantage of p-values, objectivity, is not achieved in practice. If this is the criticism that bothers you, the alternative is to make methodologies more rigorous, require authors to submit more information and reduce the conventional significance threshold for p-values. In my opinion this is going in the wrong direction. ",
            "date": "Updated March 21, 2020",
            "views": "425",
            "upvotes": " View 210 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Peter Flom",
                    "user_href": "/profile/Peter-Flom"
                },
                {
                    "user_id": "Nate Garton",
                    "user_href": "/profile/Nate-Garton-1"
                },
                {
                    "user_id": "Erik Anson",
                    "user_href": "/profile/Erik-Anson"
                },
                {
                    "user_id": "Matt Mosteiro",
                    "user_href": "/profile/Matt-Mosteiro"
                },
                {
                    "user_id": "S. Bhatnagar",
                    "user_href": "/profile/S-Bhatnagar-7"
                },
                {
                    "user_id": "Bolu Aiyepola",
                    "user_href": "/profile/Bolu-Aiyepola"
                },
                {
                    "user_id": "Robin Saunders",
                    "user_href": "/profile/Robin-Saunders"
                },
                {
                    "user_id": "Jitesh Gurav",
                    "user_href": "/profile/Jitesh-Gurav"
                },
                {
                    "user_id": "Prateek Karandikar",
                    "user_href": "/profile/Prateek-Karandikar"
                },
                {
                    "user_id": "Sawood Bokhari",
                    "user_href": "/profile/Sawood-Bokhari"
                }
            ]
        }
    ]
}