{
    "title": "What are the signs of a bad scientific paper? - Quora",
    "tags": [],
    "response": [
        {
            "author_info": {
                "name": "Vadim Zaytsev",
                "href": "/profile/Vadim-Zaytsev"
            },
            "answer_text": "Information delivery fail. I mean, we can talk all we want about the other bits and pieces which are more fundamental and important, but in the end we all agree that if the paper is incomprehensible due to bad structuring of the material, or really bad grammar, or badly chosen examples, or anything like that, then the readers will not be able to understand it correctly. And if they do not understand, they also cannot appropriately assess it, identify its contributions and rely on them.Lack of \u201cmeat\u201d. If structuring the paper with all the necessary sections, subsections, figures, listings, tables, diagrams, theorems and proofs form its skeleton, then there is also \u201cmeat\u201d on those bones. Bad papers fill the space between the bare bones and the allowed paper limit by \u201cfat\u201d: endless examples, rantings, reintroductions of old definitions, poinless discussions, unstructured side remarks, etc. Good papers have enough \u201cmeat\u201d: that is, if you really understand what is going on in the paper, you can explain it to another expert in the same area in two minutes, and they will go \u201coh, that\u2019s interesting!\u201d. It can be a new method, a nontrivially proven theorem, an unexpected experimental result, solid reasoning behind the methodology, etc. All real contributions of the paper are \u201cmeat\u201d.Vintage. Each paper needs to present a new result: something novel, something that has never been done before. If the paper begins with a statement that the authors have made some minuscule progress with respect to their previous dozen of papers and combined previously existing methods in a boring predictable way, you start getting the vibe that it will be somewhat bad. A lot of good research is incremental, but we really do not care much about every little refinement.Ease of replication. Don\u2019t get me wrong, replicability is very important. In computer science, it is always a bonus point if you provide source code listings or put them somewhere on GitHub or SourceForge. For medical research, the more open you are with your experimental setups and results, the more others can profit from it. But if the result is too easy to reproduce by a non-enthusiastic reader in half an hour, it is a bad scientific paper.Outrageous hypotheses. We all need to form a hypothesis to be proven of disproven later by the research steps, or at least most of us do. If such hypotheses are sloppily formulated, or contradict formal theories used later in working with them, or ignore well-known limitations, or are just so restrictive that any possible outcome is bound to be trivial, then all the readers and reviewers will most probably go \u201cwhoa\u201d and will stop reading or continue only in order to find the numerous bugs and inconsistencies. Very few good papers start with outrageous assumptions and manage to justify them afterwards.",
            "date": "Answered July 9, 2012",
            "views": "45",
            "upvotes": " View 9 Upvoters",
            "upvoters": [
                {
                    "user_id": "Wajeeh Ur Rehman",
                    "user_href": "/profile/Wajeeh-Ur-Rehman"
                },
                {
                    "user_id": "Arif Perdana",
                    "user_href": "/profile/Arif-Perdana"
                },
                {
                    "user_id": "Lukasz Wrobel",
                    "user_href": "/profile/Lukasz-Wrobel"
                },
                {
                    "user_id": "Aishwary Patil",
                    "user_href": "/profile/Aishwary-Patil"
                },
                {
                    "user_id": "Aleksey Belikov",
                    "user_href": "/profile/Aleksey-Belikov-1"
                },
                {
                    "user_id": "Samira Palipana",
                    "user_href": "/profile/Samira-Palipana"
                },
                {
                    "user_id": "David A Robinson",
                    "user_href": "/profile/David-A-Robinson"
                },
                {
                    "user_id": "Evgeni Bolotin",
                    "user_href": "/profile/Evgeni-Bolotin"
                },
                {
                    "user_id": "James Pitt",
                    "user_href": "/profile/James-Pitt-1"
                }
            ]
        },
        {
            "author_info": {
                "name": "Robert Kaspar",
                "href": "/profile/Robert-Kaspar"
            },
            "answer_text": "There are two very easy ways. First, look at the number of times a paper has been cited.  Important work gets cited by other researchers.  If it's been cited a hundred times, it's a key paper in the field.  This metric doesn't work as well for recent articles (published in the last year or two). Second, look at the prestigiousness of the journal in which a paper is published.  This is often quantified by the journal's \"impact factor\", the number of citations a paper in that journal receives on average.  There are many exceptions, but as a rule of thumb papers in high-impact journals like Science or Nature (I.F. ~10) are more important than those in The Journal of Many Long Obscure Necessarily Abbreviated Words (I.F. <1).  The peer review process is stricter for prestigious journals.  However, some of the work they publish is more exciting than sound. If you want to get fancy, you can also look at a paper's reference section.  A poor paper often cites other work done by the same authors to artificially increase their citation count (see first paragraph above), even when said work isn't relevant.  Note that all authors do this to some extent, but you should be skeptical when a quarter of the references are self-citations. Edit: As Brian reminds me, the above are just heuristics and only useful for screening.  The best way by far is to read and judge for yourself the paper in question, like Peter said; I thought that might be obvious enough not to mention. ",
            "date": "Updated June 27, 2014",
            "views": "36",
            "upvotes": " View 7 Upvoters",
            "upvoters": [
                {
                    "user_id": "Carlos Zepeda Rosas",
                    "user_href": "/profile/Carlos-Zepeda-Rosas"
                },
                {
                    "user_id": "Abdulla Al Mamun",
                    "user_href": "/profile/Abdulla-Al-Mamun-30"
                },
                {
                    "user_id": "Abhinav Sirothia",
                    "user_href": "/profile/Abhinav-Sirothia"
                },
                {
                    "user_id": "Ari Zax",
                    "user_href": "/profile/Ari-Zax"
                },
                {
                    "user_id": "Varna Sri Raman",
                    "user_href": "/profile/Varna-Sri-Raman"
                },
                {
                    "user_id": "Hari Prasad",
                    "user_href": "/profile/Hari-Prasad-171"
                },
                {
                    "user_id": "Emma Yu",
                    "user_href": "/profile/Emma-Yu"
                }
            ]
        },
        {
            "author_info": {
                "name": "Tiberiu Tesileanu",
                "href": "/profile/Tiberiu-Tesileanu"
            },
            "answer_text": "This depends a bit on what you mean by 'good' paper. A paper may be excellent although its conclusions are wrong. The Einstein, Podolski, Rosen paper on what is now known as quantum entanglement might qualify: they concluded that there must be something wrong with quantum mechanics, but it turned out that local realism was instead the problem. So they were wrong, but they couldn't have known that: the experiments to prove it took several more decades to happen. Nevertheless, their ideas pushed people to think about entanglement, which might one day even help us build quantum computers. So, in general, I would call a paper 'good' if its exposition is clear and its methods are sound, regardless whether the conclusions turn out to be right or not. If you know enough about the field and have the time, the best way to assess the quality of a paper is to read it very carefully and try to reproduce its results. If you can do it, and if the arguments presented in the paper are non-trivial, it's probably a good paper. A bad paper might have shoddy logic, like ignoring important alternatives or overstating conclusions. Some of these things can be noticed even without going through every detail and calculation one by one, but my experience is that you will miss many more. If you don't have the time to read the paper carefully or if you don't have the necessary background to understand it, it's best to defer the decision to others. Have other scientists checked the work? Has anyone replicated the findings? Also, always important to ask: are there any conflicts of interest for any of the people involved? In general, the more recent the paper, and the less attention it got from other experts in the field, the less confident you should be in your assessment of it. Note that I'm not saying a paper is bad if it's not popular -- many \"forgotten\" papers have turned out to be very important. I'm saying if you lack the expertise to judge the paper yourself, and you don't know what other experts' opinions are, then you simply shouldn't have a strong opinion on the paper either way. And, one last note: don't ever trust the media coverage of a scientific paper, no matter its source. Even reputable sources typically make horrific mistakes and misrepresentations. This is partly because the reporters don't understand the work, partly because both they and the authors of the paper want the news to sound as sensational as possible. Instead, always go for the paper itself. If you can't understand the paper, and it's too recent (say, in the last 5 years) for there to have been much replication by other scientists, you should simply refrain from judgment until you have more information. ",
            "date": "Answered April 30, 2015",
            "views": "25",
            "upvotes": " View 2 Upvoters",
            "upvoters": [
                {
                    "user_id": "Caroline Wright",
                    "user_href": "/profile/Caroline-Wright"
                },
                {
                    "user_id": "Eric Wilsfvord",
                    "user_href": "/profile/Eric-Wilsfvord"
                }
            ]
        },
        {
            "author_info": {
                "name": "Konstantinos Konstantinides",
                "href": "/profile/Konstantinos-Konstantinides"
            },
            "answer_text": "The value of a paper is directly correlated to what you already know.For example, a \"tutorial\" or review paper has great value when you are entering a field, but may not be as valuable to you later on in your career. A large number of papers consist of minute variations of the authors' work. Depending on where you start reading in that \"family of papers\", the first papers you read will probably be much more valuable than later ones, since later on you will find repeated information. Journal papers go through a far more rigorous review process than conference papers. Some Journals have a higher reputation than other journals. You may want to take that into account in the beginning. As others mentioned, if you find a certain paper referenced by multiple others, it is probably an important one. In this regard, even a not so good paper may actually help you by pointing to you to some other papers that you will find extremely useful. In other words, if you try to do research, there are no short cuts. You have to search, skim, and read anything that seems related to your problem and make your own opinion. ",
            "date": "Answered May 14, 2014",
            "views": "14",
            "upvotes": " View 1 Upvoter",
            "upvoters": [
                {
                    "user_id": "Aleksey Belikov",
                    "user_href": "/profile/Aleksey-Belikov-1"
                }
            ]
        },
        {
            "author_info": {
                "name": "Lucas Dixon",
                "href": "/profile/Lucas-Dixon-1"
            },
            "answer_text": "Signs of a badly written paper: * Very long sentences. * Lots of references to other sections. * No clear description of the scientific contribution of the paper. * No clear description of related work.* Terminology is introduced, but it doesn't get used.* Terminology that is not introduced gets used.* Terminology is used inconsistently. * Emotive language that tries to \"sell\" the work, system, or ideas in the paper. * References to sections that do not exist.  While not quite the question that was asked, a more important question is perhaps: what are the signs of a bad scientific paper: * No related work/lack of context for the work being done.* It is not published in a peer reviewed journal.* No one else cites the paper. ",
            "date": "Answered July 5, 2012",
            "views": "35",
            "upvotes": " View 15 Upvoters",
            "upvoters": [
                {
                    "user_id": "CS Ihda",
                    "user_href": "/profile/CS-Ihda"
                },
                {
                    "user_id": "Peter Fletcher",
                    "user_href": "/profile/Peter-Fletcher-25"
                },
                {
                    "user_id": "Jeffrey Brender",
                    "user_href": "/profile/Jeffrey-Brender"
                },
                {
                    "user_id": "William Robb",
                    "user_href": "/profile/William-Robb-3"
                },
                {
                    "user_id": "Brandon Sheline",
                    "user_href": "/profile/Brandon-Sheline"
                },
                {
                    "user_id": "Gabbigale Stone",
                    "user_href": "/profile/Gabbigale-Stone"
                },
                {
                    "user_id": "Franck Dernoncourt",
                    "user_href": "/profile/Franck-Dernoncourt"
                },
                {
                    "user_id": "Alisa Chomhirun",
                    "user_href": "/profile/Alisa-Chomhirun"
                },
                {
                    "user_id": "Richard Sima",
                    "user_href": "/profile/Richard-Sima"
                },
                {
                    "user_id": "Ti Zhao",
                    "user_href": "/profile/Ti-Zhao"
                },
                {
                    "user_id": "Eddie Xue",
                    "user_href": "/profile/Eddie-Xue"
                },
                {
                    "user_id": "Ethan Hein",
                    "user_href": "/profile/Ethan-Hein"
                },
                {
                    "user_id": "Sanjay Sabnani",
                    "user_href": "/profile/Sanjay-Sabnani"
                },
                {
                    "user_id": "Marc Bodnick",
                    "user_href": "/profile/Marc-Bodnick"
                },
                {
                    "user_id": "Hongwan Liu",
                    "user_href": "/profile/Hongwan-Liu"
                }
            ]
        },
        {
            "author_info": {
                "name": "Caroline Wright",
                "href": "/profile/Caroline-Wright"
            },
            "answer_text": "Conclusion doesn't follow from the results of study described in the paper, conclusion not the only possible interpretation of results, conclusion not the most likely explanation for the results, or conclusion is even contradicted by the results of the study. Many cases of this type of issue are probably deliberately misleading, and this type of miswriting of a paper paves the way for the common sin of the paper abstract that misleadingly reports the opposite of what the study actually found. As many readers of papers only ever bother to or can afford to look at the abstract, this type of misrepresentation can deceive many. ",
            "date": "Answered January 24, 2014",
            "views": "972",
            "upvotes": " View 2 Upvoters",
            "upvoters": [
                {
                    "user_id": "Aleksey Belikov",
                    "user_href": "/profile/Aleksey-Belikov-1"
                },
                {
                    "user_id": "David A Robinson",
                    "user_href": "/profile/David-A-Robinson"
                }
            ]
        },
        {
            "author_info": {
                "name": "Rened Avliz",
                "href": "/profile/Rened-Avliz"
            },
            "answer_text": "Clearly, to me, is 1) the extensive use of secondary references. Instead of citing the original author you prefer to use one who summarises the ideas or data. The non intelligent use of wikipedia I include in this topic; 2) a confusing or not clear method, as not scientific support of your way to get your conclusions; 3) get conclusions out of the box, they could be nice and well written but if not well supported by the whole method, they are a kind of falsification. ",
            "date": "Answered June 28, 2012",
            "views": "409",
            "upvotes": " View 1 Upvoter",
            "upvoters": [
                {
                    "user_id": "Max Reeder",
                    "user_href": "/profile/Max-Reeder"
                }
            ]
        },
        {
            "author_info": {
                "name": "Zen Faulkes",
                "href": "/profile/Zen-Faulkes"
            },
            "answer_text": "\"Judge for yourself\" is little difficult if you are not familiar with common issues. Good questions to start with:  Is it an experiment (something was deliberately manipulated) or a study (no manipulation)?Was there a hypothesis being tested, with explicit predictions made in advance?Was there a control group? Was the control appropriate?What was the sample size? (Large is better.)Was assignment to the control and test groups randomized?Was the recording of the data double blind?Were the results analyzed statistically?Was the paper peer reviewed by other scientists?Many of these issues are talked about in Ben Goldacre's work, like Bad Science and Bad Pharma. This may also be useful (though it's a bit more about reporting than the papers themselves): A Rough Guide to Spotting Bad Science. ",
            "date": "Answered June 20, 2014",
            "views": "24",
            "upvotes": " View 11 Upvoters",
            "upvoters": [
                {
                    "user_id": "Max Reeder",
                    "user_href": "/profile/Max-Reeder"
                },
                {
                    "user_id": "Caroline Wright",
                    "user_href": "/profile/Caroline-Wright"
                },
                {
                    "user_id": "Ainy Ishaq",
                    "user_href": "/profile/Ainy-Ishaq"
                },
                {
                    "user_id": "Jeffrey Brender",
                    "user_href": "/profile/Jeffrey-Brender"
                },
                {
                    "user_id": "Walter Clayton",
                    "user_href": "/profile/Walter-Clayton"
                },
                {
                    "user_id": "Lars Nystr\u00f6m",
                    "user_href": "/profile/Lars-Nystr\u00f6m"
                },
                {
                    "user_id": "Alok Sharma",
                    "user_href": "/profile/Alok-Sharma-68"
                },
                {
                    "user_id": "Roberto Vilar",
                    "user_href": "/profile/Roberto-Vilar"
                },
                {
                    "user_id": "Huang Linqi",
                    "user_href": "/profile/Huang-Linqi"
                },
                {
                    "user_id": "William Halmeck",
                    "user_href": "/profile/William-Halmeck"
                },
                {
                    "user_id": "Tatyana Kh",
                    "user_href": "/profile/Tatyana-Kh"
                }
            ]
        },
        {
            "author_info": {
                "name": "Gio Wiederhold",
                "href": "/profile/Gio-Wiederhold"
            },
            "answer_text": "As Peter said, you learn something relevant non-obvious from reading it.But that takes time, so Robert's answer builds on other readers and reviewers.In some fields conference papers are well reviewed, and the conference volume will list the acceptance percentage.On-line publishing is muddying the traditional constraints.Many commercial edited books with paper collections are not well refereed. ",
            "date": "Answered June 20, 2014",
            "views": "15",
            "upvotes": " View 3 Upvoters",
            "upvoters": [
                {
                    "user_id": "Eliane Poungoum",
                    "user_href": "/profile/Eliane-Poungoum"
                },
                {
                    "user_id": "Edwin Khoo",
                    "user_href": "/profile/Edwin-Khoo"
                },
                {
                    "user_id": "Robert Kaspar",
                    "user_href": "/profile/Robert-Kaspar"
                }
            ]
        },
        {
            "author_info": {
                "name": "Mustapha Rady",
                "href": "/profile/Mustapha-Rady"
            },
            "answer_text": "My supervisor tell me that grammar, punctuation, the size  of the font the organizing and the accuracy of the data ",
            "date": "Answered May 24, 2012",
            "views": "291",
            "upvotes": "0"
        },
        {
            "author_info": {
                "name": "William Robb",
                "href": "/profile/William-Robb-3"
            },
            "answer_text": "It has the creationist institute logo lol. ",
            "date": "Answered February 1, 2015",
            "views": "269",
            "upvotes": " View 1 Upvoter",
            "upvoters": [
                {
                    "user_id": "Leo McDevitt",
                    "user_href": "/profile/Leo-McDevitt"
                }
            ]
        }
    ]
}