{
    "title": "How to minimize a cost function using gradient descent - Quora",
    "tags": [
        "Gradient Descent",
        "Convex Optimization",
        "Mathematical Optimization",
        "Functions (mathematics)"
    ],
    "response": [
        {
            "author_info": {
                "name": "Benjamin Obi Tayo Ph.D.",
                "href": "/profile/Benjamin-Obi-Tayo-Ph-D"
            },
            "answer_text": "Gradient Descent and its variants is one of the most important optimization algorithm used in machine learning. To illustrate its use, I built a simple python machine learning estimator that implements linear regression using gradient descent. Here is a link to the article: Machine Leaning: Python Linear Regression Estimator Using Gradient Descent This article shows how by minimizing the cost function, the algorithm learns or calculates the linear regressing parameters that can be used for curve fitting.  ",
            "date": "Answered December 26, 2018",
            "views": "677",
            "upvotes": "0"
        },
        {
            "author_info": {
                "name": "Leo Shuai",
                "href": "/profile/Leo-Shuai"
            },
            "answer_text": "x(k+1)=x(k)-td Where d is the gradient at x(k) and t is some stepsize. ",
            "date": "Answered May 4, 2017",
            "views": "440",
            "upvotes": "0"
        }
    ]
}