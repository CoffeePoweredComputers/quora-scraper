{
    "title": "Which is the best way to index 20 million pages in Google? - Quora",
    "tags": [
        "Google Search Index",
        "Google Search",
        "Search Engine Optimization (SEO)"
    ],
    "response": [
        {
            "author_info": {
                "name": "Archana Panwar",
                "href": "/profile/Archana-Panwar-1"
            },
            "answer_text": "Google Webmaster Tools allows you to request an increased crawl rate. Navigation architecture can be improved to see if you can improve access to more of your content. Look at it from a user's perspective: If it's hard for a user to find a specific piece of information, it may be hard for search engines as well. Unless you have set the crawl rate manually in Webmaster Tools then site performance will have a direct affect on the crawl rate. Only try to do this with a site that has existing authority. Make sure you use XML sitemaps, we have hundreds of them with 50k pages in each one, linked to from a sitemap index, we only need to submit the index to Webmaster Tools and all 20m+ URLs are submitted automatically. Note that this is purely server side optimisation, optimising client side scripts or CSS is pointless as Googlebot doesn't crawl these. Ensure your on page SEO and structure is spot on, avoid burying pages deep in the site Make sure you don't have duplicate content because of inconsistent URL parameters or improper use of slashes. By eliminating duplicate content, you cut down on the time Googlebot spends crawling something it has already indexed. Use related content links and in-site linking within your content whenever possible. Randomize some of your links. A sidebar with random internal content is a great pattern to use. Find ways to get external links to your content. This may accelerate the process of it getting indexed. If it's appropriate to the type of content, making it easy to share socially or through email will help with this. ",
            "date": "Answered March 14, 2017",
            "views": "857",
            "upvotes": " Answer requested by Arnav Gavde",
            "upvoters": [
                {
                    "user_id": "Yaphet Kotto",
                    "user_href": "/profile/Yaphet-Kotto-2"
                }
            ]
        },
        {
            "author_info": {
                "name": "Anika Sharma",
                "href": "/profile/Anika-Sharma-45"
            },
            "answer_text": "Set the crawl rate manually in Webmaster Tools then site performance will have a direct affect on the crawl rate.Only try to do this with a site that has existing authorityMake sure you use XML sitemaps, 50k pages in each one, linked to from a sitemap index, you only need to submit the index to Webmaster Tools and all 20m+ URLs are submitted automatically.You need to make sure the site can be downloaded as quickly as possible. Note that this is purely server side optimisation, optimising client side scripts or CSS is pointless as Googlebot doesn't crawl theseEnsure your on page SEO and structure is spot on, avoid burying pages deep in the site",
            "date": "Answered March 15, 2017",
            "views": "445",
            "upvotes": " View 1 Upvoter "
        }
    ]
}