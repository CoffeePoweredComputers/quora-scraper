{
    "title": "Does NASA add color to space photos? - Quora",
    "tags": [
        "Astrophotography",
        "Color Theory",
        "NASA",
        "Space Travel",
        "Colors (vision)",
        "Space Exploration",
        "Outer Space",
        "Photographs",
        "Photography"
    ],
    "response": [
        {
            "author_info": {
                "name": "John Haynes",
                "href": "/profile/John-Haynes-8"
            },
            "answer_text": "I see questions like this fairly frequently. The answer isn't, no pun intended, black and white. Nearly all research telescopes, such as the Hubble Space Telescope (HST) use monochrome cameras. Monochrome cameras are what people often refer to as black and white. In fact, all cameras are monochrome cameras at their base level. Color is produced by capturing images through filters. In the case of color cameras, the filters are built-in. I'll circle back to this. The sensor in all digital cameras, from those on the HST, the upcoming James Webb Space Telescope (JWST), a high end Canon or Nikon DSLR, or even your iPhone all work in basically the same way: they count photons. When a photon of light - a photon being the smallest unit of light - reaches the image sensor, it causes the charge on the image sensor to increase slightly. Each sensor is made up of a number of individual pixels, essentially individual sensor elements, arrayed in a grid. When the exposure is started, the sensor starts letting the charge build up, much like a bucket left in the rain to collect water. When the exposure ends, it looks at the charge to determine how much light was collected. This number is converted into a number. There are two main types of image sensor: Charge-Coupled Devices (CCD) and Complementary Metal-Oxide Semiconductors (CMOS). While there are differences, they work fundamentally the same: they count photons. Typically, CCD devices record light in 16 bit values while CMOS are typically 14 bit sensors. A bit, of course, is a single unit of data: either a 1 or 0. 2 bits is two units which produce up to 4 values: 00, 01, 10, 11. 3 bits is 8 values, 4 is 16.... 14 bits offers up to 16,384 values, or values between 0 and 16,383. When recording values from an image sensor, 0 would be black and 16,383 would be white. Every value in between is a shade of gray. 1 is not black... it's very close to black, but it's actually a very, very dark shade of gray. Every image is made up of these values, which then determine how bright to make a pixel on the screen. An image file is basically just a collection of these values. In the case of color images, all colors are created by mixing red, green, and blue light. A color image file actually contains three values per pixel: the brightness levels in red, green, and blue. For a monochrome camera, such as the main imaging cameras on the HST, to create a standard color image, three separate images are captured: one each through red, green, and blue filters. These are then combined to create the color image. For cameras that produce color images on their own, what are known as one-shot-color, or OSC, cameras, they have a grid of filters installed directly on-top of the image sensor. This is called a Bayer matrix. Typically every group of four pixels, arranged 2x2, is covered by one red, two green, and one blue filter. When the image is taken, the values are saved to the file as RGB values. But, again, that's not how most astronomical cameras work. The main reason for this is sensitivity: whenever you pass light through a filter, you block some light. Astronomical cameras want to capture as much as possible. Not only that, but astronomical cameras are often used to record very specific colors of light. What we experience as color is a matter of the wavelength of the light received. Light with shorter wavelengths, around 400 nm (nanometers), is bluer while light with longer wavelengths, approaching 700 nm, is redder. We also know that specific wavelengths are produced by certain elements and chemicals. You may have learned in high school science class that when you burn copper it gives off a green flame. Other substances produce other colors. If you can identify the specific colors making up the light from a star, nebula, or galaxy you can determine what elements are present. One common example is found with Hydrogen. When an electron in a hydrogen atom falls from its second-highest to its third-highest energy state, it releases a photon of light with a wavelength of 656.28 nm. We call this Hydrogen alpha and it is a deep red. When you see this specific wavelength, you know hydrogen is present. One of the filters on the HST is what we call a Hydrogen Alpha, or H-alpha, filter. It blocks all light except that coming in at or very close to 656.28 nm. The HST, as well as other astro-imaging systems, often capture H-alpha light to determine if hydrogen is present. Filters that capture a specific band like this are known as narrow-band filters. There are several common wavelengths used in narrow-band imaging. H-alpha is probably the most common, but so are filters for ionized sulfur (SII) and doubly-ionized Oxygen, or OIII. these three colors are commonly captured and then used to create images, such as the well-known Pillars of Creation image. SII has two associated wavelengths: 671.1 and 673.0 nm. OIII also has two emission lines, at 500.7 nm and 495.9 nm. For both SII and OIII, their dual emission lines are close enough together that one filter is usually used to capture them. The natural color of OII would be in the light-blue/cyan range. For H-Alpha, its red. For SII it's a deeper red. For an image like the Pillars of Creation, this would pose a problem. So what is typically done is the colors are shifted. Just a moment on this. When a monochrome camera takes an image, regardless of what filter is in place, the sensor only sees brightness. What makes it a color is how it's treated afterward. If you look at an image taken through an OIII filter, it's a monochrome image. An image taken through a red filter, not a narrow-band filter, is also going to be monochrome. The key to understand here is that the brightness values are the brightness values of what passed through the filter. So, when the image is processed, a color is assigned to the data. For the Pillars of Creation, the OIII data was assigned blue, H-alpha was assigned green, and SII was assigned red. When mixed together, they form the well-known image. Now, this is where people like to accuse NASA or anyone creating astrophotographic images of faking the colors. But that's not entirely fair. It's not quite faking it, it's adjusting it. If you've ever seen an image taken through an infrared camera, the same kind of technique is used. We cannot see infrared light, but the camera can. For us to see it, the wavelengths are just decreased to bring it into visible range. There are telescopes out there that are capturing infrared images, ultraviolet images, x-ray images, and more. None of those are visible light images, but we can see the images because the data is scaled to make it visible. This is essentially what is being done. Without such adjustment, typically we wouldn't see what's there to be seen. It's by filtering and re-combining the light that we actually can make out what's there to be seen. So when you ask if NASA is adding color... the answer is yes and no. The images are representations of the light that's received, but have to be adjusted in order to make them visible and make sense. ",
            "date": "Answered March 24, 2020",
            "views": "725",
            "upvotes": " View 14 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Roy Wilson",
                    "user_href": "/profile/Roy-Wilson-64"
                },
                {
                    "user_id": "David Brodeur",
                    "user_href": "/profile/David-Brodeur"
                },
                {
                    "user_id": "Milan Minic",
                    "user_href": "/profile/Milan-Minic-2"
                },
                {
                    "user_id": "Tim Cole",
                    "user_href": "/profile/Tim-Cole-7"
                },
                {
                    "user_id": "Norris Jones",
                    "user_href": "/profile/Norris-Jones-15"
                },
                {
                    "user_id": "Sam Bailey",
                    "user_href": "/profile/Sam-Bailey-195"
                },
                {
                    "user_id": "Abhishek Grewal ( \u0905\u092d\u093f\u0937\u0947\u0915 \u0917\u094d\u0930\u0947\u0935\u093e\u0932)",
                    "user_href": "/profile/Abhishek-Grewal-\u0905\u092d\u093f\u0937\u0947\u0915-\u0917\u094d\u0930\u0947\u0935\u093e\u0932"
                },
                {
                    "user_id": "Kushagra Sharma",
                    "user_href": "/profile/Kushagra-Sharma-151"
                },
                {
                    "user_id": "Yoni Amatya",
                    "user_href": "/profile/Yoni-Amatya"
                },
                {
                    "user_id": "Rajat V n",
                    "user_href": "/profile/Rajat-V-n"
                },
                {
                    "user_id": "Kanji Banji",
                    "user_href": "/profile/Kanji-Banji"
                },
                {
                    "user_id": "Franjo Zezelj",
                    "user_href": "/profile/Franjo-Zezelj"
                },
                {
                    "user_id": "Jayesh Desai",
                    "user_href": "/profile/Jayesh-Desai-32"
                },
                {
                    "user_id": "Ava Dena",
                    "user_href": "/profile/Ava-Dena"
                }
            ]
        },
        {
            "author_info": {
                "name": "Michael Mozina",
                "href": "/profile/Michael-Mozina"
            },
            "answer_text": "It depends on the image. If you\u2019re talking about SDO solar images, they are almost always adding a (false) color to what is otherwise simply a black/white image because the photons in those specific wavelengths are not usually in the visible spectrum. Anything that is outside of the visible spectrum is usually given a \u2018false color\u2019 of some kind. Hubble images on the other hand are sometimes in the visible spectrum to start with, and NASA isn\u2019t assigning an artificial color that isn\u2019t already there in some instances. It really depends on the wavelength and the equipment. Any photon that isn\u2019t in the visible spectrum is usually assigned an artificial color, whereas most photons that fall in the visible spectrum are typically not altered. ",
            "date": "Answered March 25, 2020",
            "views": "370",
            "upvotes": "0"
        },
        {
            "author_info": {
                "name": "Rob Pettengill",
                "href": "/profile/Rob-Pettengill"
            },
            "answer_text": "No, but the do reveal colors that our eyes cannot see. This happens in two ways: our eyes cannot see color in dim light, NASA images often brighten colors already in images so that we cannot see them.NASA cameras capture colors that humans cannot see, outside the range of visual wavelengths. These colors are \u201cassigned\u201d to colors we can see in that are not used otherwiseThese are not added colors, but rather real color data that has to be transformed to be perceived by human eyes ",
            "date": "Answered March 23, 2020",
            "views": "456",
            "upvotes": " View 4 Upvoters",
            "upvoters": [
                {
                    "user_id": "Paul Maas",
                    "user_href": "/profile/Paul-Maas-9"
                },
                {
                    "user_id": "Nicholas Liaskos",
                    "user_href": "/profile/Nicholas-Liaskos"
                },
                {
                    "user_id": "Nikki Zee",
                    "user_href": "/profile/Nikki-Zee-1"
                },
                {
                    "user_id": "Tanya Chandra",
                    "user_href": "/profile/Tanya-Chandra-6"
                }
            ]
        },
        {
            "author_info": {
                "name": "Tom Nathe",
                "href": "/profile/Tom-Nathe"
            },
            "answer_text": "All color cameras, even your eyes add color. Filters for Color Imaging and for Science Color gamut \u2013 Page 2 \u2013 dot color For cameras, it\u2019s called the Bayer Filter. But simply, it\u2019s a system of filters that break up the incoming light into it Red, Green, and Blue components as individual pixels. The imager then reads off each pixel, and then transmits them off to memory, or on a viewscreen. Take a magnifying glass and look at a color monitor to see it in action.  For astronomical photography, an image is taken multiple times through different colored (Red, Green, and Blue) filters onto a monochrome imager. Each of the RGB images is then later reassembled into one composite photo that gives off the color. HubbleSite - Behind the Pictures - Meaning of Color ",
            "date": "Answered March 23, 2020",
            "views": "711",
            "upvotes": " View 5 Upvoters",
            "upvoters": [
                {
                    "user_id": "Tim Cole",
                    "user_href": "/profile/Tim-Cole-7"
                },
                {
                    "user_id": "Darren Bord",
                    "user_href": "/profile/Darren-Bord"
                },
                {
                    "user_id": "Nicholas Liaskos",
                    "user_href": "/profile/Nicholas-Liaskos"
                },
                {
                    "user_id": "Nikki Zee",
                    "user_href": "/profile/Nikki-Zee-1"
                },
                {
                    "user_id": "Ivan Sanchez",
                    "user_href": "/profile/Ivan-Sanchez-18"
                }
            ]
        },
        {
            "author_info": {
                "name": "Tim Cole",
                "href": "/profile/Tim-Cole-7"
            },
            "answer_text": "I\u2019ve answered questions like this several times. As others have noted, it\u2019s a tricky question to answer. All color images, even the ones you take with your smartphone camera, are synthetic. When you make images using wavelengths that we can\u2019t see, such as ultraviolet light or microwaves, you have to render them in visible shades. Here\u2019s one post where I describe the process in more detail: Tim Cole's answer to Is outer space as colorful as often seen in photos, or are the photos falsely colored to clarify what it is we're looking at? ",
            "date": "Answered March 24, 2020",
            "views": "118",
            "upvotes": " View 2 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Glenn Rager",
                    "user_href": "/profile/Glenn-Rager"
                },
                {
                    "user_id": "Gordon Dewis",
                    "user_href": "/profile/Gordon-Dewis"
                }
            ]
        },
        {
            "author_info": {
                "name": "Robert Mudry",
                "href": "/profile/Robert-Mudry"
            },
            "answer_text": "The \u201cartsy\u201d photos are composites taken from multiple telescopes using various EM wavelengths. The \u201ccolor\u201d is often added to highlight what objects would look like if we could see microwave, UV, IR and/or gamma waves. BTW - the pretty photos are strictly for us civilians. Pro astronomers pretty much use signal analysis and data processing to do their real job. The photos are PR for congress-critters who aren\u2019t bright enough to understand how astronomers actually work. ",
            "date": "Answered March 23, 2020",
            "views": "408",
            "upvotes": " View 3 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Gary Nicholls",
                    "user_href": "/profile/Gary-Nicholls-11"
                },
                {
                    "user_id": "Nikki Zee",
                    "user_href": "/profile/Nikki-Zee-1"
                },
                {
                    "user_id": "Nicholas Liaskos",
                    "user_href": "/profile/Nicholas-Liaskos"
                }
            ]
        }
    ]
}