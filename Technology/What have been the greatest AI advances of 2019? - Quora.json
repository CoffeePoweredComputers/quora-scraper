{
    "title": "What have been the greatest AI advances of 2019? - Quora",
    "tags": [
        "2019 (year)",
        "Artificial General Intelligence",
        "Artificial Intelligence"
    ],
    "response": [
        {
            "author_info": {
                "name": "Xavier Amatriain",
                "href": "/profile/Xavier-Amatriain"
            },
            "answer_text": "This year started with a big recognition to the impact of Deep Learning when Hinton, Bengio, and Lecun were awarded the Turing award. You might think that after a few years of neck-breaking speed in innovation, this kind of recognition might be signaling that we are getting near some sort of plateau. Well, think again. That is nowhere in sight yet. It is true that hot areas have clearly shifted and while a few years ago image recognition was all the rave this year we have seen more impressive advances in language. I will go into all of this below, but let me start by summarizing the biggest headlines of AI in 2019, in my own very biased opinion: Computers learn to talk (i.e. language models like Bert and specially GPT-2 get scaringly good)AI becoming good at creating synthetic content has some serious consequencesThe biggest theoretical controversy continues to be how to incorporate innate knowledge or structure into machine learned models. There has been little practical progress towards this end, and little progress towards any other theoretical breakthrough.The revolution may get unsupervised at some point, but for now we can make it self-supervisedComputers continue to get better at playing games and can now collaborate in multi-agent escenariosOther areas like Healthcare and Recommender Systems continue to see advances by using Deep Learning, but some of these advances are questionedThe war between frameworks continues, with a major TensorFlow release and also big movements on the Pytorch arena. But, let\u2019s get right into it and dive into each of these fascinating 2019 headlines. The year of the Language Models I think it is hard to argue against the fact that this has been the year of Deep Learning and NLP. Or more concretely, the year of language models. Or even more concretely the year of Transformers and GPT-2. Yes, it might be hard to believe, but it has been less than a year since OpenAI first released talked about their GPT-2 language model. That blog post sparked a lot of discussions about AI safety since OpenAI did not feel comfortable releasing the model. Since then, the model was publicly replicated (see here and here), and finally released. However, this has not been the only advance in this space. We have seen Google publish AlBERT, XLNET, and Universal Transformers, and also talk about how BERT has been the largest improvement to Google search in years. Besides Google, most of the other big players in the AI space have also published their own language models: Salesforce, Amazon, Microsoft, or Facebook seem to all have really bought into the Language Model revolution.  What can these models do in practice? Besides the obvious and scary \u201cgenerate credible fake tweets\u201d, there are much more constructive ones that we have seen during this past year. For example, Google told us how they were using them not only for search as mentioned above, but also for their Smart compose feature. Facebook learned models to answer questions wholistically and Allen Institute\u2019s Aristo AI passed an eighth-grade science test. In fact, if we look at the SQUAD leaderboard, it seems nowadays anyone can surpass human-level reading comprehension by combining some of these known approaches (see image below).  I do expect to see many more impressive advances in this space in 2020 as it seems we are getting closer and closer to passing the Turing Test and having computers that \u201ccan speak human\u201d. That being said, we should also temper our expectations since there have been many papers that have also identified the limitations of the current approaches. To start with, Google\u2019s impressive study sets a good backdrop on the limitations of transfer learning in language. In Limitations of Language Models for generating text or storytellers, the Stanford NLP folks walk us through situations when these language models work, and many others where they don\u2019t. Of course, a key aspect of these limitations is the fact that these models are expected to generalize across a wide range of tasks and even domains. However, we know that, as shown in \u201c To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks\u201d , it usually pays off to fine tune models to specific tasks. Our team at Curai came to that exact conclusion when comparing general language models to those trained on the medical domain in \u201c Domain-Relevant Embeddings for Medical Question Similarity \u201d. So, we are still far from having general-purpose language models that can tell good stories and adapt to different tasks and domains. Finally, I could not finish this paragraph on limitations of language models without mentioning Merity\u2019s great \u201c Stop Thinking With Your Head \u201d where he shows how for many tasks a simple LSTM model can perform almost as well as the most complicated Transformer. Combining knowledge/structure In 2019 we continued to hear loud voices advocating for AI not to get stuck in a Deep local maxima. According to many, me included, we should be able to combine data-intensive deep learning approaches with more knowledge-intensive methods to add some form of innate structure. While it is true that there is a lot of work to be done in that space, we did see many examples of research combining deep learning and more \u201ctraditional\u201d AI. In \u201c Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems \u201d, Salesforce presents a state-of-the-art approach for slot-filling task-oriented dialogue systems that combines deep learning with more traditional conversational methods. \u201c Neural Assistant: Joint Action Prediction, Response Generation, and Latent Knowledge Reasoning \u201d is a recent paper from Google that also combines several deep and knowledge-intensive approaches for the same purpose. Wizard of Wikipedia: Knowledge-Powered Conversational agents is Facebook\u2019s response in that same space. \u201c ERNIE: A knowledge graph-enhanced language model \u201d is a novel approach where a language model is trained not only on natural language data, but also a knowledge graph. Interestingly though, it turns out that while language models might benefit from being trained on knowledge graphs, they themselves also encode knowledge and can be used as knowledge bases (see \u201c Language Models as Knowledge Bases? \u201d). Similarly, deep learning models like BERT or ELMO do not only encode knowledge, but also syntax. The Stanford NLP team showed how syntax trees can be directly inferred from such models.  The self-supervised revolution If I had to name two important fundamental trends behind many of the advances highlighted above those would be: transfer learning, and self-supervision. Transfer learning (the idea that you can train a model on an original dataset and apply the resulting model elsewhere) is a pretty obvious idea behind language models, but also earlier image models trained on Imagenet and the like. The idea of self-supervision might be a bit less obvious. Maybe that is why some are calling it The Quiet Revolution despite Yan LeCun having screamed it out loud for years to anyone who was listening. In any case, self-supervision, the idea that you can train a model on unlabelled data by exploiting the context in the data itself, is catching on. Not only language models like BERT or ALBERT use the idea extensively, but this same notion is being applied to other domains, making it easier to train on large corpuses without needing to spend huge efforts in annotation. For example, self-supervision is being used to improve image classification models. See for example \u201c Self-Supervised Learning of Pretext-Invariant Representations\u201d , \u201c Data-Efficient Image Recognition with Contrastive Predictive Coding \u201d, or the recent \u201c Self-training with Noisy Student improves ImageNet classification \u201d. All of these approaches improve on SOTA supervised methods while using much less labeled data.  A fascinating application of self-supervision that takes the idea a step further is Facebook\u2019s \u201c Unsupervised Question Answering by Cloze Translation \u201d where they split the question answering problem into two steps. The first steps generates synthetic training data with a model that synthesizes fill-in-the-gap questions from documents. The second step uses a traditional Q&A model. This is similar to our \u201c Learning from the experts \u201d where we sidestep the need for costly and noisy labeling of medical data by generating synthetic training data. Other miscellaneous research advances The year also came with other advances that don\u2019t neatly fit into the main trends of combining knowledge with deep learning, or self-supervision. What follows are some of my favorite highlights in this miscellaneous category. In \u201c The Lottery ticket hypothesis \u201d the authors show a fascinating result: due to sheer chance, some subnetworks with many less parameters than the original network have comparable accuracy. For some reason their connections have initial weights that result in a much more effective training. The authors also present an algorithm to identify those \u201cwinning tickets\u201d. In the same vein of finding more efficient yet performing models, \u201c EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks \u201d introduces an approach to uniformly scale all dimensions in a CNN. Rectified Adam is a variation over the well-known Adam optimizer that results in better training and higher accuracy (if you don\u2019t know Adam, you probably should since, according to Chip Huyen, it\u2019s the most commonly asked question during interviews). In \u201c Classification Accuracy Score for Conditional Generative Models \u201d the authors present a new way to evaluate generative models by training a classifier on synthetic data but predicting labels on real data. While this is not strictly speaking a novel idea (see e.g. \u201c LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation \u201d), and the paper applies it only on GANs for image, it does show an interesting path for evaluating other generative models in different domains. In \u201c Training on Synthetic Noise Improves Robustness to Natural Noise in Machine Translation \u201d we see how injecting synthetic noise during training can improve the quality of the trained models. I personally believe that the use of synthetic data plus noise is going to bring a lot of advances in AI in the near future. But, maybe I am just biased because our own publication \u201c Learning from the experts: From expert systems to machine-learned diagnosis models \u201d already proposed a combination of these two techniques. Another important line of research is on how to apply learned models in \u201cthe wild\u201d by modeling uncertainty and out-of-distribution modeling. In real-life it is important to understand the uncertainty of model predictions and whether the data point is outside of the distribution on which the model was trained on. A few papers on this space have been presented at NeurIPS 2019 (see e.g. \u201d Modeling Uncertainty by Learning a Hierarchy of Deep Neural Connections \u201d and \u201c Likelihood Ratios for Out-of-Distribution Detection\u201d . This is also a very important aspect to tackle in healthcare, and it is indeed the focus of our recent paper \u201c Open Set Medical Diagnosis \u201d. Finally, I should note that there have been a lot of publications in the broad space of what I would call human-AI-Interaction that includes research areas like fairness, bias, or interpretability. Hard to pick the most impactful works in this space, but I will highlight two with almost opposite takeaways. AI2 presented AllenNLP-Interpret, a toolkit for interactive model interpretations and explanations. This work won the EMNLP best demo award. On the other extreme, in \u201c Manipulating and Measuring Model Interpretability \u201d Microsoft researchers surprisingly concluded that model transparency and interpretability not only did not help, but could hamper user ability to detect model mistakes. And one last, and very recent, piece of news on this space of human-AI is Facebook\u2019s announcement of a $1M deep-fake detection challenge. Clearly detecting fake content is going to be a huge deal in the future, and it is good to see that we are already putting efforts into this. Let\u2019s keep playing It has been more than 3 years since Alpha Go beat Lee Sedol, but we are still receiving the aftershocks of such a feat with Sedol recently announcing his retirement because of that defeat. And, while it might seem like there is not much more progress to be made in AI for games, computers insist on getting better at more, and more complex games. This year we saw two major feats, with DeepMind reaching human-level performance in Quake III Arena Capture the Flag and wining the Starcraft competition with AlphaStar. Both these advances show us the ability of algorithms not only to master complicated but highly structured games like Go, but also to adapt to more fuzzy strategic goals in which even collaboration is needed.  A final, and pretty recent, advance in this space is Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model where Deepmind again shows like a combination of search and a learned model can be applied to gain superhuman performance not only in a single game, but in a range of games. Is there room for deep outside of text and image? Of course the Deep revolution is impacting way beyond text and image. I will focus on the two areas I follow most closely: recommender systems and healthcare. Interestingly, I have seen a similar pattern in both areas this year (warning: you should know that us \u201cscientists\u201d see patterns all around us). In recommender systems, Deep Learning has been impacting the research community for some time now, probably ever since Youtube published their first paper on using Deep Learning for recommendations in 2016. Maybe unsurprisingly, most of the results related to deep learning continue to come from industry continues. My former team at Netflix has definitely jumped on the DL train, and they have been speaking publicly about different deep learning enhancements to the Netflix recommender system. See this recent tutorial by Anoop Deoras on using Deep Learning for recommendations. Even Facebook, who are honestly not very active in the recommendation arena, made quite a splash this year by open-sourcing a Deep Learning recsys model/framework. But, not everything is shiny and bright on the deep side of the recsys street. As a matter of fact, the best paper award at the Recsys conference went to a paper that questions most of the recent advances in using deep learning approaches and shows how simpler methods obtain similar to better results.  In healthcare, the deep learning revolution has already been here for some time too. There have been many research papers in this intersection. So much, that Stanford/Google felt like we are at a point when we can even publish \u201c A guide to deep learning in healthcare \u201d. Truth be told, the most interesting/credible applications of deep learning to healthcare are actually still when applied to either images or text (see some of our own examples in \u201c Domain-Relevant Embeddings for Medical Question Similarity \u201d or \u201c Prototypical Clustering Networks for Dermatological Disease Diagnosis \u201d). However, when applied to more complex data like Electronic Health Records (EHR) we show that much simpler models perform just as well as deep neural networks (see our upcoming \u201cThe accuracy vs. coverage trade-off in patient-facing diagnosis models\u201d). Frameworks/Platforms Unsurprisingly, the \u201cAI framework war\u201d that I already mentioned in last year\u2019s round up has not cooled down. The two main contenders continue to be Google\u2019s TensorFlow and Facebook\u2019s Pytorch. Who will win the war remains to be seen, but according to some data, Pytorch continues to win the research battle, while TensorFlow dominates in production-ready systems. This last year, TensorFlow released the highly-anticipated TF 2.0, its main highlights being tight integration with Keras, default eager execution mode, and more Pythonic function execution. In other words, TF is trying to become more usable and friendly\u2026 more Pytorch-like. TensorFlow also introduced its Swift library, which immediately attracted many programmers. Even fast.ai \u00b7 Making neural nets uncool again announced that it would embrace Swift, and maybe question the use of Pytorch over time, which would be a huge deal. On the other hand, Pytorch has continued to evolve. Probably the most notable news late this year was another popular library Chainer merging into Pytorch.  There was a lot of movement outside of the two big players too. Microsoft recently announced a really interesting serving infrastructure, which provides highly efficient production-ready serving of models trained with any framework supporting the ONNX standard. This can be a huge win for Pytorch since its serving infrastructure is lagging behind TensorFlow\u2019s for sure. On the NLP-frameworks arena, I have to obviously mention the great work by the folks at Huggingface who seem to release the code to any NLP advance before the paper even hits arxiv. A huge accomplishment especially given that all big players are interested in this space. Even Uber published their own framework for dialogue system research.  ",
            "date": "Updated January 9, 2020",
            "views": "233",
            "upvotes": " View 245 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Dhinesh Dharman",
                    "user_href": "/profile/Dhinesh-Dharman"
                },
                {
                    "user_id": "Dmitriy Genzel",
                    "user_href": "/profile/Dmitriy-Genzel"
                },
                {
                    "user_id": "Claudio Martella",
                    "user_href": "/profile/Claudio-Martella"
                },
                {
                    "user_id": "Vijay Tatkar",
                    "user_href": "/profile/Vijay-Tatkar"
                },
                {
                    "user_id": "Paulo Sobral",
                    "user_href": "/profile/Paulo-Sobral-8"
                },
                {
                    "user_id": "Murilo Queiroz",
                    "user_href": "/profile/Murilo-Queiroz"
                },
                {
                    "user_id": "Tiehui Chen",
                    "user_href": "/profile/Tiehui-Chen"
                },
                {
                    "user_id": "Antonio Fernandez Alonso",
                    "user_href": "/profile/Antonio-Fernandez-Alonso"
                },
                {
                    "user_id": "Jeff Bruce",
                    "user_href": "/profile/Jeff-Bruce-50"
                },
                {
                    "user_id": "Aniket Gurav",
                    "user_href": "/profile/Aniket-Gurav-2"
                }
            ]
        },
        {
            "author_info": {
                "name": "Soumya Prakash Pradhan",
                "href": "/profile/Soumya-Prakash-Pradhan-2"
            },
            "answer_text": " Over the years, artificial intelligence has amazed everyone with numerous breakthroughs, and this year it was no different. I bring to you the top 7 amazing AI advancements that changed the world forever. 1. Robot Hand\u2019s Dexterity OpenAI\u2019s successfully trained a robot hand called Dactyl that adopted to the real-world environment in solving the Rubik\u2019s cube. The robot was entirely trained in the simulated environment but was able to transfer the knowledge into a new situation successfully. OpenAI used automatic domain randomisation technique and improved the capabilities of hand for solving the Rubik\u2019s cube. Although Dactyl solved the cube. 2. Deepfake \u2013 Bringing Picture To Life Samsung, in May, created a system that can transform facial images into video sequences. They used the generative adversarial network (GAN) to create deep fake videos just by taking one picture as input. Researchers from Samsung used high-fidelity natural image synthesis for enabling ML models to resonate realistic human expression. 3. AI-Generated Synthetic Text OpenAI, in February, released a small model called Generative Pre-Training (GPT) to generate synthetic text automatically. The firm eventually released the full version of the model, GPT-2, in November. The model was trained with over 8 million web pages, resulting in creating content that was difficult to determine whether it was a generic or synthetic text. 4. Gamification Of Memories Google\u2019s DeepMind changed the way reinforcement learning works to gamify memory. To allow AI agents to make better decisions in the present, they used Temporal Value Transport (TVT) for sending lessons from the future. This allowed the agent to understand the long term consequences of decisions that can be taken at present. Although the methodology was carried out in a game, it was unprecedented in the AI landscape. 5. Solving Three-Body Problem The three-body problem was one of the longest-standing predicaments in the scientific community. Precisely identifying the future position of objects has numerous use cases, especially in the space, determining the position of heavenly bodies allows scientists their research. The researchers of Edinburgh used neural networks to pinpoint the future location, thereby, opening up the door for extending it into an n-body problem. 6. Upside Down Reinforcement Learning A team from Swiss AI Lab introduced a new methodology calling it an upside-down reinforcement learning. They successfully carried out reinforcement learning in the form of supervised learning. This allowed the team to provide rewards as input, which is contrary to how the traditional reinforcement learning work. Such a technique also enabled them to imitate and train the robot for carrying out strenuous tasks just by imitating in front of a machine. The command-based methodology assists ML models in expediting the training process, thereby decreasing the time required in the AI workflows. 7. Explainable AI AI is making great strides, but understanding the methodologies inside the black box is crucial for bringing thrust. Therefore, different companies released services to allow businesses to underline the prime factors that lead to outcomes from their machine learning models. For the first time, firms are able to clear the cloud and gaining insights into the way the black box works. Although one cannot yet obtain all the aspects of a conclusion from models, it has a more prominent role to play in democratising AI. So, did you learn something that you didn't know before, If Yes then Upvote & Share \ud83d\ude4f ",
            "date": "Answered December 22, 2019",
            "views": "182",
            "upvotes": " View 2 Upvoters",
            "upvoters": [
                {
                    "user_id": "Brett Gardner",
                    "user_href": "/profile/Brett-Gardner-31"
                },
                {
                    "user_id": "Ava Bishop",
                    "user_href": "/profile/Ava-Bishop-3"
                }
            ]
        }
    ]
}