{
    "title": "What major improvement will be made to cameras in the future? - Quora",
    "tags": [
        "Digital Cameras",
        "Invention and Inventions",
        "Cameras"
    ],
    "response": [
        {
            "author_info": {
                "name": "Dave Haynie",
                "href": "/profile/Dave-Haynie"
            },
            "answer_text": "Well, let\u2019s take a look at where we ar, first. The cutting edge of what everyone\u2019s doing in camera design today suggests what we\u2019ll all have in time. And where it might also be headed. Since You Mentioned Color First of all, you\u2019re way off on color film. There were experiements in color photography in the 1840s. Gabriel Lippmann invented a system of color photography using interference in the 1890s, which resulted in his winning the Nobel Prize in Physics in 1908. James Clerk Maxwell came up with the RGB color system we use to this day in many things back in 1855. By 1861, some experimenters were making color photographs using color separation (three separate photos shot through R, G, and B filters.. the basis for the early versions of the Technicolor cinema process, years later). This idea was used in various forms, and more or less met the modern age 70 years ago, when Kodak introduced Kodachrome in 1935, and Agfa introduced Agfacolor in 1936.  An interesting thing about color film for ages is that it\u2019s a stack of color sensitive layers. Films typically had three layers, red, green, and blue sensitive layers. Most digital sensors work differently. A silicon photodiode, the sensing element in modern digital cameras, are not color sensitive. In order to get color, you typically have a series of filters over the sensor array, most of which follow the idea invented at Kodak by Bryce Bayer, and thus dubbed the Bayer filter. Color is created on a per-pixel basis in a Bayer sensor by \u201cdemosaicing\u201d .. in short, nearby colors are interpolated to deliver the \u201cmissing\u201d two colors for each pixel location. This works very well for most situations, but can result in false colors along very sharp contrast lines. The filters are also blocking nearly 2/3 of the light per pixel in order to deliver the color filtering.  Some attempts around this have been made. The Foveon sensor, owned by Sigma Corporation, works in a stack, kind of the same idea as film. These sensors use photodiodes at three different levels in the silicon substrate. As light passes through silicon, there\u2019s some natural absorbtion\u2026 you\u2019ve no doubt seen this in water if you\u2019re done much snorkling or underwater photography. So their diagram illustrates blue, green, and red sensor layers.. but what they actually capture is red, red+green, and red+green+blue (aka, white) layers, and use proprietary subtractive processing to deliver the final image. Sounds nice in theory, but in practice, it has issues. The lower layers get much less light, so much less that in the current Foveon sensors, the top layer is at full resolution, the second and third layers are at 1/4 resolution. So there\u2019s still some interpolation. And these sensors are fairly low in sensitivity compared to Bayer sensors with a similar effective resolution. However, a number of other companies have been looking at different solutions to this approach that might get around some of the problems with multi-layer sensors. Part of this is based on the way the eye works\u2026 we have only about 6 million color-sensitive sensors in our eyes, but 120 million luma-sensitive sensors (see Dave Haynie's answer to How many pixels for a human eye?). So we just don\u2019t care about color as much as resolution, but we also do care about color distortions in a photo. So improving today\u2019s cameras on color, other than in basic and slow refinements to how we do it today, can have unwanted side effects.  Another technology in development for digital image color is micro color splitting. As mentioned, we\u2019re currently filtering away as much as 2/3 of our color using the Bayer filter. In this approach, micro deflectors are used which transmit some light, redirect other light to adjacent pixels, based on the light\u2019s color, using diffraction. This would still require interpolation, and more complex interpolation than in a straight Bayer sensor, but it\u2019s allowing more light into the sensor. Another approach in development, dubbed \u201cactive pixel color sampling\u201d, is using a moveable color filter array. This would not be in a classic Bayer arrangement, but include R, G, and B colors in line with each other. The sensor would shoot three shots, each of which has the sensor in a different position, delivering R, G, and B color information to each sensor location across three exposures. The issue, of course, is making this fast enough to deal with motion between frames\u2026 and we\u2019ll see in a bit, this approach is actually already being done by some cameras is a slightly different way. Resolution and Sensitiviy When the digital photography revolution more or less began, I was shooting film and scanning it into my PC at about 10MPixels. The interesting thing was that just about everything I shot showed visible grain in the digital image. Except Kodachrome 25. No significant grain. So I used that as one benchmark for chemical photography. Kodak claimed about 100 line-pair per millimeter, or 200 lines per millimeter. Simply representing that in pixels gets you a 7200 x 4800 pixel image, or 34.5 megapixels. It\u2019s not quite that simple. Looking structurally, chemical films have had individual silver halide crystals in a consistent but random pattern. Not all crystals are of the same size, but they typically vary from 0.5\u03bcm to 5\u03bcm or so, depending on the kind of film, sensitivity, etc. The crystals are of different sizes and randomly distributed across the film. In color photography, you have this across your three color sensitive layers, and in addition, during the development process, the film chemically forms dye clouds around each developed silver particle, then the silver is bleached out. So there\u2019s some loss of resolution there. When a photon encouters a silver halide crystal, a free-carrier electron is released within the crystal. That electronic passes through the halide crystal and will be trapped at a \u201csensitivity speck\u201d on the crystal, where it\u2019s likely to reduce a silver ion in the halide lattice to an atom of metallic silver. One about four silver atoms are thus collected, typical photo developers will be able to convert the entire crystal to metallic silver. Thus, the size of the crystal determines the size and number of these \u201cphoton targets\u201d, and the sensitivty and grain structure of the film. In digital, you typically have a fixed array of photodiodes. The photodiodes in my Canon 6D\u2019s 20Mpixel full frame 35mm sensor are 6.55\u03bcm. But move to a Canon 5Ds, and you have a 50Mpixel full frame sensor with 4.14\u00b5m pixels\u2026 still much larger than the size of many silver halide crystals. These very sensitive and relatively large sensors have very effectively extended the range of photography to the detection of a single photon in photodiodes, and given that a single pixel can capture a wide dynamic range, based on size of the pixel, the capacity of the charge well that stores the output of that pixel, and the noise in system. Great strides have been made over the years in controlling on-sensor noise. This all adds up to the fact that typical digital cameras greatly exceed the ISO equivalent of film and deliver less noisy images when matched shot for shot. There\u2019s no grain per se in digital imaging, but there is noise. Some noise is inevitable shot-noise\u2026 simply, when you\u2019re shooting in very low light, there can be a scarcity of pixels enough to result in a grainy look. There is also thermal noise, so as shot goes longer, more thermal noise can become visibile (which is why astrophotographers use cameras with sensor cooling). And finally, read noise, the process of reading out the pixels itself creates noise. Most modern camera sensors do the analog-to-digital conversion on the actual image sensor these days, to minimize noise in the system from affecting the image.  So where might these things go? There have been experiments in using different kinds of pixel arrangements. Fujifilm\u2019s EXR sensor used a modified Bayer array that made it easier to \u201cbin\u201d pixels together. So the 12Mpixel sensor in my Fujifilm X-S1 can be configured as a 6Mpixel sensor with effectively 2x larger pixels, as the diagonally-adjacent R, G, and B pixel pair work together. As more capabilities go onto sensor chips, we may expect to see other kinds of reconfiguration on-sensor.  Another approach has been to build an array of both large and small photodiodes. This creates an even more complex interpolation, but is one way to deliver an increased dynamic range. Fujifilm also worked on this, and others have been looking at the concept. One stumbling block is what you do to deliver a final image. Digital photography pretty much demands final output that\u2019s an N x M array of RGB pixels (or Y Cr Cb pixels if you\u2019re shooting in JPEG, but that\u2019s expanded to RGB when loaded). Some of these in the past failed simply because the image information was too large for the preceived value. That\u2019s likely to change as we start to stop caring about file sizes. And back to thinking about pixel sizes\u2026 we can actually make really, really small pixels. Many smartphones have camera sensors with pixels in the 1\u00b5m range. The problems there are twofold. For one, there\u2019s still the issue that a smaller pixel is a smaller target. Modern chip technologies like BSI and Stacked sensors have pushed the \u201cpixel fill\u201d, the amount of the actual space for a pixel that\u2019s filled by a photodiode, to nearly the theoretical maximum (some space has to be left to avoid crosstalk). Reducing on-chip noise makes up a little, but ultimately, a smaller pixel is going to be a problem as the sun goes down. So when you have the pixel space, as in a full frame 35mm camera, smaller pixels are not always a big win. That said, Canon recently showed off a prototype of a 120Mpixel sensor camera. No announced deliver, but that is over twice the resolution of the Canon 5Ds available today. The most obvious issue with this, and where we\u2019re going to hit the limits of physics, is on diffraction. When you buy that shiny new smartphone with a 16\u201320Mpixel sensor, you probably also get a lens on that bad-boy that\u2019s at f/1.8 or so. And fixed. You can\u2019t go to a smaller aperture. The reason for that is simple: due to diffraction, the property of light through an aperture to bend a little, any stopping down of that smartphone lens and you\u2019d get a loss of practical resolution. For a DSLR like the Canon 5Ds or a smaller sensor pro camera like a Micro Four-thirds Olympus OM-D, you don\u2019t really want to stop down much below f/11. You also need better lenses for smaller pixels. When Canon came out with the 5Ds, they included a list of existing lenses recommended for it. After all, if your lens isn\u2019t sharp enough to deliver a true 50Mpixel image, what\u2019s the point? Other Sensor Technologies We made the jump from CCD to CMOS some years ago, and CCD was the big improvement over the Videcon tube. So there\u2019s a fair chance that silicon isn\u2019t the final destination for photography. Our CMOS sensors improved their light collection with backside illumination (BSI), and then with stacked sensors, which put the \u201cbackside\u201d sensors on their own chip, and stack that on top of the chip that does all the digital processing. Right now, we\u2019re pushing the speed up on these sensors with additional MIPI interface lanes and internal parallelism, having dumped the analog interface and done all the digital processing on-chip, which is now actually not the chip that does the image sensing anymore, in the stacked configuration. Next thing, we have some kind of on-chip memory. They did that in CCDs, ages ago. Each sensor in a CCD had its own memory, which stored charge and shifted that out, cell to cell, in an analog interface.. that was the \u201ccharge coupling\u201d, essentially a bucket brigade of charge. But it meant you had to read the whole chip before taking the next shot, and that without a mechanical shutter, the image was subject to interference. They fixed this by basically taking every-other pixel and using that as memory. So the shot is snapped, and every pixel charged is sent to the charge bucket next door, which doesn\u2019t have a sensor on it. These could then proceed to bucket-brigade in peace, and the CCD had it\u2019s global electronic sensor. Most CMOS sensors today have \u201crolling\u201d shutters. The electronic shutter is really just a matter of enabling capture on the chip, stopping it, reading it out. This \u201crolls\u201d on a CMOS chip as the senaor is sensitized, timed, and read out precisely to get the correct exposure. Pro cameras have mechanical shutters, still, to prevent the small problem of objects moving in this time. So some now, but in the future mainsteam cameras will have global shutters: the sensor is exposed, and every pixel is digitized and transferred to on-sensor memory instantly, then sent out as quickly as possible over MIPI or some proprietary digital interface. And still, we\u2019re talking about silicon here, just more of it. But some companies are looking at using elements of organic compound in otherwise silicon sensors in order to improve on the sensor design. There may be a day when the sensor parts or even the whole chip is based on carbon compounds instead of silicon. Carbon is also being researched for making still-smaller chips, which is certainly a concern for on-camera capabilities. As we get to 5nm chips, we\u2019re actually geting to too-few silicon atoms per transistor. Carbon transistors can be smaller, so that along makes this an interesting direction. One interesting possibility with organic sensors is a dramatic increase in signal-to-noise ratio and dynamic range. Right now, we get sensors with 12 to 14-bits worth of resolution, but once we get to 22\u201324-bits, we\u2019ll have a true full-range ISO-less camera. Keep in mind, ISO is kind of artificial in still cameras. In video, there has traditionally been an amplifier between the analog output and the analog-to-digital converter (ADC), which was set in terms of actual amplification in decibels (dB), basically a volume control\u2026 you turned it up if the signal was too weak, down if it was too strong. The problem is, your camera\u2019s 12dB isn\u2019t necessarily my camera\u2019s 12dB. So when making digital cameras, the industry and the International Standards Organization (ISO) got together and established guidelines for ISO for cameras. And most of that was about setting a scale on that volume control that corresponded in some expected way to the scale used on film. Over time, camera companies offered \u201cextended\u201d ISO ranges that might use software processing to deliver a higher setting, rather than actual hardware. That\u2019s because you can only amplify a signal so much without making the noise visible. And even having that extra amplifer staget there before the ADC added a little noise. So some cameras, today, are \u201cISO-less\u201d .. they digitize the 12-bits or 14-bits out of the sensor and use that for all ISO ranges. If you had a 14-bit sensor and only shot JPEG, for example, you\u2019d have an honest 6 ISO range on that camera, without overextending the image processing. But imagine a sensor that put out 20 or 24-bits of infomation. You\u2019d have crazy dynamic range, the camera would record that whole 24-bits, and you\u2019d decide in Photoshop what ISO you wanted to shoot at! Where Do I Put It All? Right now, we have a kind of golden age of photography. My Canon 6D has only about 4.5fps shooting and a memory buffer that can fill up. But on my newer Olympus OM-D EM-5 mk II, I can shoot raw photos all day (well, until the memory card fills or the battery needs replacing) at 5fps, with full image stabilization and autofocus. A new Olympus model extends that to 18fps with electronic shutter, or (buffer limited) up to 60fps with first-frame-only AF. That same new model, the E-M1 mk II can also shoot a series of 14 shots before your finger actually depresses the shutter button all the way. That\u2019s tapping today\u2019s super fast SDXC UHS-II memory cards, and some cameras use even faster CFast (SATA-based), XQD (PCI Express x1 based) and other new technologies. That will continue\u2026 memory speed will get faster and faster. The smallest card I have today is 32GB, I have a 256GB memory card in my smartphone. This may all be leading to the day when the average consumer camera doesn\u2019t have a memory card at all\u2026 just a nearly unlimited chunk of memory that you\u2019ll probably never use, but is cheap enough to not worry about. Right now, we use Flash memory, which has been the memory of choice for just about the whole digital photo revolution. Ok, Sony used floppied, then 3\u2033 CDs on the early Mavicas (though the first one was analog electronic photography, not digital). But that could be coming to an end. Several kind of memory are in the works that can replace both DRAM and Flash memory. Imagine that all your camera\u2019s memory runs at buffer speeeds. You could shoot that 60fps until the battery runs down and not worry about it. I don\u2019t expect professionals will be keeping all their photos on their cameras forever. But things may change to the point that you typically don\u2019t use a memory card, it\u2019s all in internal non-volatile MRAM or FeRAM or PRAM or maybe NanoRAM or 3D X-Point RAM or Memristors. When you enter your house or office, the camera and the home network talk and instantly just start unloading the camera. Could be there\u2019s an SD card in there, but one that just functions as an optional running backup to the internal memory. And yet, we still want archival storage. Today, I put my photos on a RAID for quick access and onto HTL Blu-ray discs for long term backup. But a 50GB Blu-ray isn\u2019t going to make you happy when you\u2019re dealing in too many 100MB or even GB sized photos. Hard drives are useful, too, but they also wear out, the break when you drop them, etc. Some folks like M-Disc, which is very reliable but expensive (a 25-pack of 100GB BDXL M-Discs costs about $400). There are new long-term storage technologies in the works, like Memristors, which may eventually provide more reliable, higher density card and disc storage. Software Aided Photography A big part of recent innovations in photography these days are a combination of clever software with your hardware, in ways that would have been impossible in the past. I expect this to continue. Since I use one, look at Olympus. They have a bunch of cool ideas in their cameras. First of all, the in-body-image-stabilization (IBIS) system. Pentax has used that as well, and more recently Sony, but Olympus pushed it to about 5 stops of stablization in the OM-D E-M5II and 5.5 in the E-M1II. Since it couples with an optically stabilized lens as well, that can deliver 6.5 stops of stabilization. People are actually taking sharp images, hand-held, with multi-second exposures. It\u2019s kind of science fiction already. I can shoot video on my E-M5II as stable as my Panasonic pro camcorder on my Glidecam 2000. Or better. This kind of technology has come very far in the last decade, and it\u2019s likely to be spread (Panasonic\u2019s using it in some new models) and improved.  Olympus used the same moving image sensor for a different trick. Because the moving sensor can be driven to offset motion, what if instead, they move it precisely to improve image quality. That\u2019s the Olympus \u201cHires\u201d mode on the E-M5II, the Pen F, and the new E-M1II. What they do is shoot eight shots, moving the sensor either 1/2 or 1 full pixel each time. That results in eight 16MPixel shots in an E-M5II raw file, but that really amounts to two 16Pixel images, offset by 1/2 pixel, with each pixel containing RGBG information. So no de-Bayering, improved color, and improved dynamic range\u2026 and that\u2019d before you figure out how to make it a single image. I mentioned the need for a nice even array before, eh? Here we have in essence one 32Mpixel full RGB image, but with interstitial pixels. Interpolating this to a rectangular regular pixel array yields twice the pixels, or 64Mpixels. Now, this won\u2019t have the resolution of a true 64MPixel camera\u2026 in-camera, a JPEG of this is rendered at 40Mpixels, and compares reasonably well to a 36Mpixel Nikon \u2014 maybe a bit less practical resolution, but better color. Now, today, this has to be done on a tripod and is only really practical for a still subject. But add in faster shooting and faster processing, and perhaps this kind of shot will work hand-held one day. There are of course other multi-shot enhanements. In signal processing, it\u2019s common to multi-sample a repeating signal to reduce visible noise. The noise we can\u2019t fully control in a system is random noise, but it\u2019s random. When I add one random number to another, I get a random number that looks exactly the same. So when I average four shots of the same thing together, I get a shot with only 1/4 of the original noise. My Canon 6D, already stellar at low-light performance, has a mode that does this. Many folks are into High Dynamic Range shooting these days, and some cameras have an auto-HDR mode that does multiple exposures to \u201cstack\u201d several for HDR processing\u2026 if you bracket three shots by 2 f-stops, you\u2019re extending your camera\u2019s natural range by 4 f-stops. Many cameras can even process this for you. That sort of thing is going to continue evolving. Some cameras can even do this today in one shot, because of the dynamic range of the sensor. For consumers, you\u2019re shooting JPEG most of the time, which records 8-bits/color (and sometimes less than that), but a top notch DSLR sensor can deliver 14-bits of information per sensor. So compressing that (which is what HDR software does) into a JPEG can give you a taste of that wide dynamic range, stuffed into something you can use in normal photo workflows.  Another recent software innovation is automatic focus stacking. Imagine you\u2019re shooting a macro shot of Mr. Frog here. You don\u2019t mind part of him being out of focus, but it\u2019s impossible to get enough of him in focus with the macro lens and exposure you need to shoot. Automatic focus bracketing will have your camera shoot a series of shots with slightly different focus. In-camera focus stacking will process the image to include the clearest part of each image. You easily get a shot that would have been difficult manually and impossible without some kind of focus stacking.  Another one from Olympus in recent cameras is Live Composite mode. This basically stacks light. It shoots multiple exposures, but only adds the increases in light between shots. This is one you can do in Photoshop by creating a Smart Object and setting the right composite mode, but it\u2019s actually more powerful to have in-camera, because you know it\u2019s done right at the point of shooting. Panasonic has a cool technology called Depth from Defocus (DFD). In the DSLR world, you have special focusing sensors in your camera that read off or through the mirror and use phase detection for auto focus. Phase detection is basically looking at a bit of image and determines not just if it\u2019s in focus, but if not, how far it\u2019s off and in what direction. This allows for very fast focusing, which most mirrorless cameras can\u2019t match. Some mirrorless put PDAF sensors on the image chip iself, but Panasonic had a different idea. They did fairly extentive analysis of their lenses and how they looked when out of focus at different points, and included that information in the lens. The camera does imagine processing on the image to get a signature that\u2019s compared to the lens\u2019s DFD information, and that allows the camera to estimate where to move the lens to get it close to focus, at which point the contrast-detection auto focus takes over. CDAF is not normally able to estimate direction or extent of defocus, so it has to \u201cseek\u201d for focus, and that\u2019s often slower than PDAF, sometimes significantly. As we have more on-camera signal processing, I think we\u2019ll see more of these novel soft of solutions. Metafeatures: Growing New Features Way back when then Canon 5D mark II shipped and started creating a buzz around using a DSLR for serious video work, Canon did something pretty unusual: they updated the camera with a whole new feature, a 24fps progressive video setting, the film standard, of course. Everyone who bought one could get this feature for the trouble of downloading. A couple of my Olympus cameras have had \u201cmajor software updates\u201d that added features. For example, when I bought my OM-D E-M5II, it didn\u2019t have a Focus Bracketing mode. Now it does. I think Fujifilm has offered updates with new features. Most cameras out today can be pretty easily updated with new software.. Olympus includes a tool that knows your gear and will tell you about updates. Panasonic came out with a V-Log update for their DMC-GH4. In this case, you had to pay $99 for this feature, but with low-contrast being an important thing for shooting video that will later be graded, having this available is a big win over no availability. And of course, lots of cameras have \u201cart filters\u201d, which do the same kinds of image processing in-camera that you get in Photoshop and other image manipulation programs. But different cameras, different art filters. I wouldn\u2019t be surprised that if, at some point, these become plug-ins\u2026 you could buy a pack of your favorite effects and put them on your camera. In fact, you more or less can today if you\u2019re using one of the few P&S cameras that\u2019s driven by Android. Or a smartphone\u2026 lots of folks shoot to Instagram and apply the filters, post-shot but right on the scene. And don\u2019t expect just silly stuff for Instagram\u2026 imagine being able to dial in and tweak lens flare or bokeh or other things you might actualy want to use as a serious photographer. Or dial in Kodacolor, Velvia, Tri-X, etc. if you wanted a specific film look, right in-camera. And also, hacks. Twenyt years ago, hacking your camera for new features might have conjured up images of a precision machine shop, but it\u2019s been more software. The older Panasonic GH3 was a very popular hacking target, as it could be tweaked to shoot at much higher bitrates, thus lower compression, than the stock software allowed. The big hack out there today is Magic Lantern, for some of the Canon cameras. This adds a whole toolbox of extra features to your Canon DSLR\u2026 some can even shoot raw video, now. As we start to use software-enhanced photography on a regular basis, we\u2019ll start to rely on it, and then get annoyed when going between camera vendors or even models that don\u2019t have those functions. The solution here may be opening cameras to third parties, maybe even with \u201ccamera API\u201d becoming standard, to let folks manipulate the image without the potential of damaging the hardware. We\u2019ll see. Into Other Dimensions All of these things so far are still discussing fairly conventional digital cameras. But some of these are changing already in interesting ways.  A number of companies are messing with curved image sensors. We already deal with one every day \u2014 the eye. The human eye has a roughly 22mm diameter, a bit larger than the sensor in a Micro four-thirds camera. But it\u2019s semi-spherical, so the actual surface area is around 1100mm^2, larger than a 35mm camera sensor. And the lens, too, is semi-spherical. So the eye can deliver a very good image without the need for the complex lenses we make for cameras today - a bit part of those many-element lens is the tick of keeping an image clear and equally bright across the flat image field we\u2019re projecting. A curved sensor would offer a better image and larger sensor in a smaller package.  Taking a completely opposite approach, why not make a flat lens? Researchers in metamaterials have come up with a flat lens made in a process similar to that of an IC. They start with a super-thin piece of transparent quartz, and build nanostructures of titanium dioxide only 600nm tall. The orientation and other physical characteristics of these structures. The characteristics are fairly narrowband right now, but they can design these with very aberations, design the specifically for projection on flat surfaces, etc. A single lens of this type may prove all you need for a camera lens.. the prototype done at Harvard, used as a microscope lens, was 30% sharper than the microscope lens it replaced. These lenses are so thin they\u2019d need to be reinforced, may times thinner than a filter. A camera could have several of these built-in, probably even a smartphone, switching them based on the desired focal length. Another new direction is the plenoptic camera, also called the light-field camera. When we think of a camera, it\u2019s easy to understand in the digital realm, we take a photo through a lens and that image is projected across an array of sensors, each of which more or less becomes a pixel in the image, a picture element. What if we actually recorded light rays? Rather than just recording an intensity for each pixel, how about an intensity and a vector \u2014 the direction of that light ray. That\u2019s the idea of the light field camera. If you\u2019re capable of doing that in any meaningful way, you essentially don\u2019t record an image, you record the information necessary to create an image in a varity of ways. A company called Lytro has done a few of these cameras, but they\u2019ve been little more than a curiosity. But now they\u2019re working on lightfield cameras for digital cinematography, which could be very interesting. And another is object processing. Right now, most new cameras can recognize human faces. Software on social media sites like Facebook can recognize good images of specific people in the photos you upload. Imagine this kind of technology baking for a few decades. Long ago, the MPEG-4 video format was designed for object processed video. So for example, maybe you get to see the football match free, but you have to pay a bit extra to see the ball. That\u2019s maybe a sill exanple, and no one\u2019s done a great deal with it, but it was discussed. Ok, no imagine your camera is smart enough to catalog many of the things in every photo. You take that shot, you get back home, load up that image into Photoshop 2030, and you see there\u2019s an ugly telephone poll listed. You go to the objects menu, delete the telephone poll by name, and that is gone from your photo, no trace left. You might also search your photo catalog for every photo of your cat \u201cJoffrey\u201d (my wife picked the name). Sure, today you can add that metadata when you go though your photo collection. But in the future, your camera may be calling out individual objects in the photo, easier because it\u2019s looking before and after the actual exposure to held define those objects, and your cataloging software would fill in more specific information: it knows there\u2019s a cat, a specific cat name Joffrey, maybe additional information, it knows he\u2019s sitting or sleeping or jumping, etc. All with very little work on your part.  We\u2019ve seen a few multi-sensor smartphones lately, but this one, the Light camera, was out before that and kind of over-the-top. This camera includes sixteen separate sensors, up to ten of which can be used for any given shot. Of the sixteen cellphone-like small sensors, five are wide angle, five are short telephoto, and six are long telephone. The light path of the all tele sensors is folded, kind of like a periscope, which keeps the camera both thin. They\u2019re doing a variety of computional photography tricks in this camera. They\u2019re compositing the output of multiple sensors to deliver a larger image with less noise. They\u2019re doing some parallex computation, based on different sensor position, which can be used to deliver artificial depth-of-field effects\u2026 as mentioned a way\u2019s back, these small sensors have very deep focal fields and can\u2019t be stopped down without diffraction. This can also interpolate between camera focal lengths to deliver something like the effect of a smooth zoom. This model was actually quite a bit more expensive than a good entry-level DSLR or mirrorless rig, but they\u2019re first production run sold out, and they\u2019re planning to make more in 2017.  Some of this kind of computational photography will certainly hit the mainstream. Already, LG, Apple, Huawei, and perhaps others have put dual cameras in their smartphones. This is somewhat an admission that today\u2019s 12\u201320Mpixel phone cameras are just not going to be any better, other than maybe by small incrementl technology improvements, without getting too large for a phone. So everyone\u2019s looking to do something new with multiple cameras. The Leica-codesigned dual-camera in the Huawei P9, while kind of an underperformer, had lots of interesting ideas. It\u2019s got two matched camera/lens systems, except that one of the cameras has no Bayer filter. So you have a monochrome shot that\u2019s 3x or so the sensitivity of your color camera. Even as close as they are, the software can exploit parallex between the two to judge the distance of things and deliver a \u201cvirtual\u201d aperture setting, doing software defocusing of background images scaled to about what you might see with a DSLR. The Slow March Some of these things seem pretty exciting to me, but there\u2019s always a wait for cool new \u201cfuture\u201d technology to make it into the present. And then, once here, a longer time for the industry and users to decide if it\u2019s a great innovation or a goofy thing to mock in some worst-of-the-year list. Innovation, after all, isn\u2019t what I\u2019m doing today, it\u2019s what history has to judge about what I did yesterday. Some things catch on fast. Image stabilization was in higher-end camera lenses for awhile, but it made its way into just about every P&S camera, camcorder, and more recently, smartphone, within only about the last ten years. And with several different ways to achieve it. Before that, it was autofocus and before that, auto-exposure. Go back to 1960s SLRs and only Konica was pushing auto exposure. Some of these things are pretty experimental, and could be dead ends, or just take years to develop. It took about 25 years from the first Lithium Ion battery working in the lab to becoming a regular battery you could buy, and another 10 for it to absolutely dominate the camera battery market. One big reason \u2014 the early ones exploded.. a problem we have seen hasn\u2019t always been resolved even today. ",
            "date": "Answered November 6, 2016",
            "views": "14",
            "upvotes": " View 9 Upvoters",
            "upvoters": [
                {
                    "user_id": "Curt Edmonds",
                    "user_href": "/profile/Curt-Edmonds"
                },
                {
                    "user_id": "Robert Shanks",
                    "user_href": "/profile/Robert-Shanks-2"
                },
                {
                    "user_id": "Pasha Aulia Muhammad",
                    "user_href": "/profile/Pasha-Aulia-Muhammad"
                },
                {
                    "user_id": "Erik Young",
                    "user_href": "/profile/Erik-Young-11"
                },
                {
                    "user_id": "Alon Poleg-Polsky",
                    "user_href": "/profile/Alon-Poleg-Polsky-1"
                },
                {
                    "user_id": "Lorenzo Peroni",
                    "user_href": "/profile/Lorenzo-Peroni"
                },
                {
                    "user_id": "Scott Danzig",
                    "user_href": "/profile/Scott-Danzig"
                },
                {
                    "user_id": "David Wiernicki",
                    "user_href": "/profile/David-Wiernicki"
                },
                {
                    "user_id": "Simon Hayes",
                    "user_href": "/profile/Simon-Hayes-3"
                }
            ]
        },
        {
            "author_info": {
                "name": "Richard Brown",
                "href": "/profile/Richard-Brown-188"
            },
            "answer_text": "With gigapixel chips already in use in astrophotography, and the essential technologies of photography going on 200 years old, with optics being older still, what is left to improve in this era of digital cameras? Well of course, the pragmatic (high) cost of digital photography plagues one and all. But there is a more pressing issue. Storage. Right now, storage technologies are tiny and temporary. Properly washed black and white negatives will outlast everything digital - currently. Digital photography has yet to break out of its tiny and temporary roots. What is happening? Well the most stable storage we currently have is MDISC, like a super, etched in stone (literally) DVD or Blu-Ray which has a theoretical life of 1,000 or more years. But it is still tiny, with a 50GB limit.  I have been patiently waiting for CARBON-based storage to make it to market. This nanotechnological breakthrough will end the \u201cspinning disk\u201d technology which began in the 1960\u2019s and has yet to achieve anything in terms of size and permanence. Carbon Storage will kill spinning disks faster than CDs killed LPs. This year, Apple replaced the 1960\u2019s born \u201cfunction keys\u201d with a vastly more powerful future: the Touch Bar. As Apple innovates, so does the rest of the computer world copy, and soon, all keyboards will be Touch Bar keyboards. Goodbye, function keys.  Here in my satellite office in Florida, I have a spinning hard disk shipping box filled with raw hard disks I use like bigger floppy disks in a combination of a single Thunderbolt reader with far more bandwidth than the disks themselves, and a pair of single disk docks under the very slow USB 3 protocol. That\u2019s a total of 4 external disks, which are generally 4\u20136 Terabytes each. 24 terabytes. In a digital universe where cell phones shoot 4K video, and my DSLRs include a digital Hasselblad at 50MB, which means single frames are well over 100 megabytes. Little hard disks are becoming a little annoying.   (The unit above is only somewhat similar in form factor to what I use.) Carbon storage will bring PETABYTES to the world, and finally, some size to our mega storage needs. It also is archival in a way no carbon disk could EVER lay claim. But carbon storage is a technology in its infancy. So we wait in a temporary, tiny, slow way. And with the pragmatic cost of having to buy 4, 6, or 8 terabyte hard disks until, finally, they will be replaced, with the further expense of TIME offloading your personal needs. I expect to spend a few months replacing more than a petabyte of spinning storage to a small portion of the first carbon drive, and get two more for the future, a primary and its backup. Now, I searched a bit for the latest on carbon storage, but work intrudes. When I first heard of it, it was about 2006, a long time by Internet nanoseconds. The other thing which would be useful in the future would be consolidation of industrial vision technologies so camera companies would have a simpler time in choosing their chips. Very few camera manufacturers have anything to do with their chip design. ",
            "date": "Answered November 5, 2016",
            "views": "526",
            "upvotes": " View 2 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Gary Elmer",
                    "user_href": "/profile/Gary-Elmer"
                },
                {
                    "user_id": "Mark Antony Rubin Klinkardt",
                    "user_href": "/profile/Mark-Antony-Rubin-Klinkardt"
                }
            ]
        }
    ]
}