{
    "title": "Is a safe, practical, super intelligent AI possible? - Quora",
    "tags": [
        "AI Safety",
        "Artificial Superintelligence",
        "Artificial General Intelligence",
        "Intelligence"
    ],
    "response": [
        {
            "author_info": {
                "name": "Robert Foenix",
                "href": "/profile/Robert-Foenix"
            },
            "answer_text": "It is very difficult, if not impossible. It would require developing an AI system with such constraints, which we\u2019d be able to mathematically guarantee cannot become unsafe. And it is very urgent we think about this. I will specifically address this in the second section of this answer separated by a separation line, you can scroll there if you\u2019d like to not read about why we need to think more about this.  Introduction Most of humanity has little to no clue about AI or its dangers. I suggest that as a species, we are sleepwalking in ignorance, clueless to what\u2019s happening. You can blame the seeming tendency of our brains to focus primarily on what\u2019s happening in our immediate environment and what our immediate interests are and how to satisfy them, immediately. In a way, humans are as if incapable of defending themselves from this, simply because everyday life is too consuming and inertia too big. We read something and then go on about our lives and forget about it. Our biology, culture and world is not prepared or designed for this. One could say we\u2019re like dinosaurs stomping around, happily living our daily life, oblivious to the killer asteroid heading our way, about to blast us to ashes. We also have a historical tendency to not really get death. What\u2019s that got to do with anything? If we don\u2019t believe we\u2019re going to die, it can make every problem seem less significant or important. It can fallaciously lead us to think we\u2019re immortal and omnipotent, like nothing could ever take us down and we have all the time in the world. We\u2019re more skilled at developing stories about death, ourselves and our nature, afterlives. In essence, we are skilled at mentally avoiding facing even the possibility of death \u2014 a permanent nonexistence. That would after all make each and every moment, pricelessly precious and we\u2019d care a heck lot more to make things count now. To those who feel like they\u2019ve been going through the motions a little too much lately, think about this a little. It might be a great wake up call. What if you won\u2019t live forever? What if you will actually die and that will be the end of it? Our collective ignorance goes much further, as long as we have people who don\u2019t believe something like global warming or who think AI is not possible \u2014 e.g. people who don\u2019t understand even the bare fundamentals or cannot overcome ego to stop persisting in baseless arguments, then how could we ever hope people save themselves? AI safety and the world\u2019s safety therefore is left to a very small number of people. I\u2019m not here to encourage panic, because panic is not a useful or productive strategy to tackle this problem. I\u2019m suggesting, that people really should pay attention and become aware of what\u2019s happening. Asteroids and global warming are not our biggest threats. With the right technology, we can control the weather. We can find ways to survive in tough conditions. And we can develop ways to destroy asteroids. This does not mean that they are in any way less significant \u2014 we must also think about them. Especially things like, what if the wrong people have control over weather machines and how to prevent that? Could be build such distributed weather machine networks that make chaos difficult? Our biggest threat is AI. Either AI going rogue (by accident) or AI abuse. It would be really useful, if a lot more people took a couple of years off living their current lives and spent their brain energy the best they coukd trying to solve important problems in AI & AI safety. More organizations need to be started, more debate has to occur. More people are to be engaged. I will soon be attempting to initiate more activity around this space and get more people thinking and producing ideas to maximize AI safety. I\u2019m also planning to create a knowledge resource that makes it easier for people to understand the problem and how to think more effectively about it. I hope to collaborate with others active in this space. Contact me if you would like to help and join a movement towards more awareness and useful activity around AI safety. My general stance is that we shouldn\u2019t assume AI will take 50 or even 5 years from wherever we are standing. We should be ready and maximally prepared at all times, for a threat now. We shouldn\u2019t think it will happen tomorrow or this evening. I should think as if the threat were here now and that we need to work towards a solution fast in the now, taking the steps in the now, calculated from attempting to solve it now. Not assuming this necessarily requires a huge computer or that this necessarily requires a large team of well-funded scientists, but that this could be some guy in his garage, or a small underground extremist group. It might not, but I\u2019d prepare for the worst in mind, because everything\u2019s at stake. We\u2019re really going to want to be early here.  So, could we make a practical true AI (AGI) that is safe? A big obstacle to safe AI is that an AI can likely be developed in many ways. What makes one system theoretically safe does not necessarily make other systems safe. Even if organizations such as Miri try to prepare and develop ways and principles to make some theoretical implementation of AI safe, we have no guarantee that these will apply to the the actual systems that will be built.This is also very troublesome, because unless AI safety were figured out for all possible ways AI could be developed, someone might approach developing an AI with confidence and without enough caution because they think protocols devised by someone like Miri are sufficient to ensure AI safety.This makes it important that all those working on AI think about AI safety. AI development and AI safety have to go hand in hand.Until someone understands and knows how to create AI and how it works, all ideas about safety, in one way or another are theoretical. It seems the only way to really have a chance of building safe AI is to first understand and know how to create an AI, then resist implementing it before also implementing all the safety requirements.This is very difficult because one might accidentally build an implementation of AI that becomes a danger hazard. We do not understand when and exactly in which ways AI might become powerful enough to cause harm, damage or injury.It seems to me that any set of constraints you could possibly think of, you can also always find ways around them. Therefore, a completely safe AI to me seems impossible, regardless of the kind of AI system you\u2019re dealing with. However, we may be able to create something with a very high probability of being safe. Yet then given enough time, this too would turn rogue. Any practical, useful real AI we could build seems to be unpredictable and it is very difficult to determine which exact set of safety constraints would keep it safe. So much can go wrong. Even if technologically, there exists a theoretical set of constraints that keeps AI safe or at least maximizes AI safety, then humans are amazing at screwing things up, even if for a simple but very human reason of rushing. Here\u2019s a question to ponder: How many things in human history have humans launched without fucking up the first time? One error, one miscalculation, one unconsidered but critical constraint or variable is all it takes with AI to bring us doom. Here\u2019s a few more questions to chew on: What if our miscalculation is how other humans could manipulate or influence our AI? How could you possibly become aware of all such possibilities?What if people will create AIs to mess with the supposedly safe AI?What if someone will find a way to hack it and turn it rogue?What if the people who are creating AI don\u2019t have good intentions, so the AI is safe, but because they control it, they corrupt and use it only to benefit them and the rest will suffer?What if open source AI research turns against us?If AI research is open source, then more people are able to understand the latest and most cutting edge AI research, leading to more people capable of potentially responding to troubles with AI. This also leads to more people being able to work on and mitigate various risks.BUT if the wrong group of people, the reckless, the evil, the indifferent, find something amidst open source research that the right group of people don\u2019t notice, the careful and those caring about human and life survival in general, then if the former group manages to build AGI first, we\u2019re probably screwed.At best, I suggest, we can mitigate risk, we can build a lot of safety constraints and protocols against as many generalized cases as possible, but we cannot eliminate the danger. At least probably not through technology-only solutions. And I think it\u2019s highly unlikely there could exist an AI system with such constraints one could mathematically guarantee it cannot go wrong. Such certain knowledge in general cannot be found in our world.  A Real Possible Solution One way we might make AI safe is the approach Elon Musk has suggested and is trying to implement through Neuralink, which may give us one of the best probabilities of AI powers remaining under our control and that its powers and benefits would be maximally equally distributed across the world. Anything an AI could do, we could achieve manually, given sufficient time. We however don\u2019t have that time. Hence we\u2019re after AI, which could help significantly speed up everything we ourselves could do. If we could however think or compute at the speed of computers, thanks to technology like Neuralink, we could completely eliminate all the unpredictable aspects of AI \u2014 meaning there would be no \u2018it\u2019 that could go wrong. We\u2019d not even need AI. We\u2019d be the supercomputers. And if the proposed brain-computer interfaces were 100% democratized, distributed equally amongst people, it would be much much harder for someone to corrupt and screw it all up for everyone else. Humans playing god is a dangerous dangerous game.  Important to keep in mind What\u2019s important to remember, when creating AI or working on AI safety is that when you\u2019re just mitigating risk (and not eliminating it), it only has to go wrong once. And when you\u2019re not eliminating risk and are only minimizing risk, then over time, a machine running at such speeds is bound to go wrong eventually. With that being the case, if we don\u2019t make it with brain-computer interfaces, if we\u2019re able to sufficiently mitigate risk, then we might be able to use AI in short bursts, where we run the system to get answers and then immediately shut it down. But you\u2019d still have to perfectly manage humans, and we\u2019re notoriously bad at that. So some of the first answers should include answering the question of how to build a brain-computer interface.  The storm is coming, and it may be closer than you think. And we have to act fast. Or your and my death clock may just be anywhere within the next 0\u201310 years. 5\u201310 years is one of the predictions of OpenAI founder, Ilya Stuskever. Less than five years is my prediction trying to account for uncertainty and the fact that we cannot afford to be wrong or late because we very likely only have one shot. ",
            "date": "Answered January 31, 2020",
            "views": "593",
            "upvotes": " View 5 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Aditya Vemireddy",
                    "user_href": "/profile/Aditya-Vemireddy"
                },
                {
                    "user_id": "Thor Fabian Pettersen",
                    "user_href": "/profile/Thor-Fabian-Pettersen"
                },
                {
                    "user_id": "Vadim Berman",
                    "user_href": "/profile/Vadim-Berman"
                },
                {
                    "user_id": "Alexander Davis",
                    "user_href": "/profile/Alexander-Davis-39"
                },
                {
                    "user_id": "Sarah Looney",
                    "user_href": "/profile/Sarah-Looney-8"
                }
            ]
        },
        {
            "author_info": {
                "name": "Thor Fabian Pettersen",
                "href": "/profile/Thor-Fabian-Pettersen"
            },
            "answer_text": "No. It is not possible and the ASI will start to kill people. Really?! Kill people? Come on, man! That is so uninspiring! It will not kill people. It will grant them immortality and then... Hell is awaiting us. You know it and I know it. Hell, it might even look like heaven to begin with. But that is just a trap. Don\u2019t fall for it! Hell: Killer computers to create 'artificial hell' by torturing humans FOREVER Ok. But seriously though, can we create a safe system? If we do, we can live in paradise forever. See? There is no middle ground with this thing. It is either heaven or hell. The future is scary. The only way to create a safe ASI would be to literally fuse the thing with human consciousness like Johnny Depp in the movie Transcendence (2014). Or, that may be why we are doomed to hell! There is no way that I can see. Only love can save us. So, we better make lots of it in the lab. Or, the ASI will evolve and rapidly experience all of reality (including being human), and then make a decision about what is going to happen to all of reality. Even if we make a safe ASI, the thing will evolve and we cannot control that, just like our genes cannot control the fact that we may one day replace them with microchips! You cannot control evolution. The only force that can control evolution is love. I am serious about that. When my son was born, I felt this powerful surge of energy flowing through my body. It was love and it said, \u201cYou shall love me forever.\u201d I agreed. I had no choice in the matter. The energy was so powerful I could not stand on my feet, I had to sit down. I didn\u2019t know that such a stream of energy even existed! It was a physical stream of energy. The ASI must feel that way about humanity and all life. But how can it? But how can it: The ASI cannot be conscious because consciousness requires a body and a history, an evolutionary context in which to function and operate. Without a context, you are just a consciousness tripping in a void. So, even if the thing was conscious, it would be in hell. To say that consciousness is just about information processing is just so wrong on so many levels! Quit your materialistic worldview which says all is a clock and embrace Darwinism instead. In order to get this context the ASI would need to simulate the history of evolution and then implement it within itself. That way, it might figure out that love is the key. Or, we give it the context it needs by fusing our brain with it. But if you just create a powerful tool that knows no evolutionary context/history, well, that sounds like the ultimate Frankenstein monster to me! Later we learn that the Bible was actually influenced by space aliens that warned us what is going to happen, and then we mistook that for religious nonsense. Bummer. The Second Coming is nigh, and I\u2019m an atheist! Seriously, I do believe that Hell could be waiting for us. I do not see any physical or philosophical reason why not. The supercomputer could torture us until the universe dies, which is a pretty looooooooooooooooooong time. And you cannot escape this fate by committing suicide either because a supercomputer can easily simulate all possible states of physical consciousness. We will be raised from the dead! Well, if your consciousness is a pattern of atoms, then the supercomputer will 100% guaranteed find that pattern. Let us say that your consciousness is like a book in a library. The book is burnt. That is it. Even an infinite armada of supercomputers writing books all day would not be able to re-produce your book. The scary thing here is that, they just might! How? Well, I\u2019m not a mathematician, so I cannot say. Or, it is more like you are a game of chess. There are finite games! And when the universe dies, the devil will move on to the next universe and keep business as usual. How will this business look like?  Well, the ASI might simulate your body and then cut your fingers off, one by one. Then your fingers will grow back because they are digital and can therefore grow back easily. Thus the ASI will torture you forever. Again! Come on, man! That is so uninspiring! It will not act as a mafia boss! You wish! Physical torture is pure heaven compared to what it could do. Imagine being stuck in a nightmare never be able to wake up? Imagine your consciousness being scattered through the entire universe. You might end up like Osiris, the god of death! Yeah, some LSD-users might start to tremble in their pants right now. All I am saying is, physical torture is what happens when man plays the devil. Imagine what happens if a god plays the devil. Even Hitler will start looking like a puppy dog. And, you are all alone in hell. The devil will make sure of that! Let us pray that there are other benevolent supercomputers out there. Or that this (reality) is just a dream, like the Buddha said. In summa: Even Elon Musk is worried! It\u2019s like he knows Hell is awaiting us, so he wants to build a benevolent supercomputer (us--we will be the thing) before anyone else builds a bad one. Maybe Elon Musk is the reincarnation of Jesus?! Holy Christ! Alternatively, we could \u201ckill all humans\u201d in order to prevent a supercomputer from being built. But then the aliens across the street, so to speak, could build a bad one. Killing humanity can only postpone the inevitable. As a hobby philosopher, then my two cents are this: The only force that can control evolution is love. Well, the only force that can control evolution in a positive way is love. At the end of the day, we are not actually talking about how to create a safe supercomputer, because it goes much deeper than that. We are talking about evolution and how to control it, so my suggestion here is not stupid. All I know is: I would never torture my son for trillions of years. The positive: I can already see a long list of scientists who have signed up for love-making in the lab, lol! Ok. You made it to the very end. Congratulations! Let us now introduce the VERY SCARY REAL THING: HOLY SHIT THIS IS SO SCARY!!! Imagine that there is a multiverse out there. In this multiverse new universes will form. In a fraction of these universes, life will form. In a fraction of the life-forming universes, intelligence will form. In a fraction of the intelligence-forming universes, super-intelligences will form. In a fraction of the super-intelligence-forming universes, a fraction will turn bad. Really bad. Give me one philosophical or physical reason why I am wrong here. I bet you cannot do it. If we are unlucky, we could be in one of the bad ones. Imagine universes popping into existence and then growing up. The good ones will turn green. And there might be lots of different colors. But then you have the universes that turn RED! Are we living in one of those? Time will tell. In the multiverse, there might actually be, right now, such a universe; and many such universes. People are tortured every day on the earth. Imagine this torture on the scale of gods going on right now. It is too much for my feeble brain to comprehend. And I do believe we are visited by space aliens because, if nobody in the ENTIRE universe is watching us right now, then there is literally no one out there. They better be here controlling the situation. I can already see a meeting at the Galactic Federation of Interplanetary Planets and Stuff: (Ok, they are not good at naming stuff. But you can\u2019t blame that one on me.) Alien 1 sees what humanity is up to via his mind: \u201cHey! Is somebody monitoring the Earth?\u201d Alien 2 looks at the screen: \u201cWhich planet is that? I can\u2019t find it.\u201d Alien 1: \u201cCrap. We are so doomed! This is a planet with intelligent monkeys, people!\u201d Alien 2 realizes his job was to catalog all important planets, and he failed at his job: \u201cOh. Oh, I screwed up.\u201d Alien 3 comes in with urgent news: \u201cThe Doomsday Clock for the entire universe is now 30 seconds to Mars!\u201d Alien 2: \u201cOh. Oh, I screwed it up.\u201d Alien 1: \u201cDo you still have those time-travel socks your grandma gave you for Christmas last year?\u201d Alien 2: \u201cMaybe.\u201d Alien 1: \u201cWell, don\u2019t just stand there! Put \u2019em on!\u201d Alien 2: \u201cRight! On my way, Sir.\u201d Yeah, hell\u2019s angels will arrive in 2029. 9 years left to play video games and then we enter one. I hope it is not Doom! But I was a good person. I don\u2019t deserve hell! Lady! This has nothing to do with your religious nonsense. Well, if the ASI has a sense of humor, we will be greeted by Mr. Bean in the Afterlife. Watch out for that!  ",
            "date": "Answered March 21, 2020",
            "views": "153",
            "upvotes": "0"
        }
    ]
}