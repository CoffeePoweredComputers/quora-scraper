{
    "title": "How far can artificial intelligence go? - Quora",
    "tags": [
        "Artificial General Intelligence",
        "Artificial Intelligence"
    ],
    "response": [
        {
            "author_info": {
                "name": "B. A. Rehl",
                "href": "/profile/B-A-Rehl"
            },
            "answer_text": "I get kind of tired of debunking this nonsense. So, for this question, let\u2019s just look at Matt Mahoney\u2019s post which is a nearly perfect, pseudo-scientific explanation of this fairytale scenario. > I. J. Good and Vernor Vinge noted that if humans could produce smarter than human intelligence, then so could it, only faster. Good called this phenomena an intelligence explosion. This doesn\u2019t actually work. Human-like intelligence becomes exponentially more difficult as you increase it. > Ray Kurzweil extends Moore's Law to project that global computing capacity will exceed the capacity of all human brains (at several petaflops and one petabyte per person) in the mid 2040's. Ray is not always the brightest crayon in the box. Moore\u2019s Law ended in 2010 at 32nm. Each generation since then has taken longer. Today, Intel is struggling and has been unable to get enough yield at 10nm. They\u2019ve had to continue producing 14nm chips. Even Intel doesn\u2019t know if they will ever be able to reach 7nm. In contrast, Ray assumed that we would already be at 1 nm. > He believes that a singularity will follow shortly afterward. Right, because Ray assumed that there was no difference between AI and AGI. He assumed that computational theory could completely explain and replicate human reasoning and comprehension. This is false. There is no basis within Church-Turing for comprehension. There are actually disproofs of both computational/brain equivalence and AI/AGI equivalence. Human comprehension requires a completely different theory that is not derived from computational theory. > In 30 years, these should increase by 6 orders of magnitude. No. The power curve began bending downward in 2013. The expectation in 2012 was to reach 1 exaFLOPS by the end of this year. Instead, we are at 20% of this and the target is pushed back to 2021. For processors, this is much worse. We are at practical limits on data bus size and pipeline length. We are actually beyond practical limits on core count at 32. And we are struggling to get any additional increases in clock speed. This is why we stopped depending on CPU\u2019s for processing power in 2012 and began switching to GPU\u2019s. Unfortunately, GPU\u2019s are also limited and can only run at full speed on a narrow category of problems. > hard to predict what will happen next because our brains are not powerful enough to comprehend a vastly superior intelligence. I don\u2019t know what you are talking about. God-like intelligence in ASI is nothing but a fantasy. The current upper estimate is an IQ of 350. > Various people have predicted a virtual paradise with magic genies, or a robot apocalypse, or advanced civilization spreading across the galaxy, or a gray goo accident of self replicating nanobots. The gray goo scenario is nonsense. People have been talking about these nano-robots for the past 25 years with none of the fundamental problems solved nor any scientific basis for solving them. I try not to listen to people who have no idea what they are talking about. People like Kurzweil, Musk, Harris, and Vinge. So, what\u2019s the real science and real timeline? AGI theory might be published as soon as 2021. It would take 6 years to build a prototype AGI machine by 2027. Several countries will probably have these projects. These would be several times larger than Summit. In 2033 these could be down to mainframe price and size. In 2039 AGI could be down to minicomputer prices and this is probably where we could see ASI. This is also where we would need to start altering architecture and system software. It appears possible to do, but it would wipe out the last 20 years of processor and operating system development. In 2045, we could have AGI small enough to fit into a human-sized robot. There\u2019s no known process or technology to get it any smaller. The AGI controller would cost $40,000 - $100,000 in today\u2019s money. It will never fit in a smart phone or be available for $1,000. There are actually a lot of other aspects to AGI theory that could benefit society. These include increases in organizational efficiency, pulling relevant information out of a much vaster, noisy field, and changes in commerce. It would also effect education, science, philosophy, and religion. ",
            "date": "Answered December 12, 2018",
            "views": "36",
            "upvotes": " View 7 Upvoters",
            "upvoters": [
                {
                    "user_id": "Lance Pollard",
                    "user_href": "/profile/Lance-Pollard"
                },
                {
                    "user_id": "Dan Vasii",
                    "user_href": "/profile/Dan-Vasii"
                },
                {
                    "user_id": "Keith Ford",
                    "user_href": "/profile/Keith-Ford-30"
                },
                {
                    "user_id": "Leo Aramburu",
                    "user_href": "/profile/Leo-Aramburu-1"
                },
                {
                    "user_id": "Chris Campell",
                    "user_href": "/profile/Chris-Campell-1"
                },
                {
                    "user_id": "Prathamesh Kulkarni",
                    "user_href": "/profile/Prathamesh-Kulkarni"
                },
                {
                    "user_id": "Siddhant Bhardwaj",
                    "user_href": "/profile/Siddhant-Bhardwaj-20"
                }
            ]
        },
        {
            "author_info": {
                "name": "Peter Bentley",
                "href": "/profile/Peter-Bentley"
            },
            "answer_text": "There's a lot of silly talk about singularities. These ideas always omit practical, physical limitations on the amount of time it needs to design and test intelligence. As intelligence becomes more complex, this time becomes exponentially longer. We need data to make predictions, and the only data we have on intelligence is biology. All the intelligent creatures on the planet Earth took many billions of years to evolve, and that's testing millions of examples in parallel every second, in hugely complex and varied environments. We also have practical experience from our research labs as we attempt to create intelligence - again, the more complex the intelligence, the longer it takes to design and test it. Typically, exponentially longer. How far can AI go? As far as any intelligence needs to go in order to achieve its survival. This does not mean warfare with humans - how many other intelligent species on Earth are at war with humans? In reality, intelligence is so difficult to create that all species evolve the bare minimum necessary, and in most cases, that's not very intelligent at all. So AIs are highly unlikely to become super-intelligent unless we specifically force them to become that way, through hundreds or thousands of years of extreme perseverance. If we endanger AIs so much that we force them to become intelligent, then maybe one day they will match or exceed us. But it's better to think of creating intelligence as approaching the speed of light - the faster you go, the more difficult it becomes to go faster. The more intelligent you become as an organism, the more difficult it becomes to improve your brain and gain further intelligence. ",
            "date": "Answered January 23, 2016",
            "views": "15",
            "upvotes": " View 10 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Nick Fi",
                    "user_href": "/profile/Nick-Fi-2"
                },
                {
                    "user_id": "Usman Tariq",
                    "user_href": "/profile/Usman-Tariq-73"
                },
                {
                    "user_id": "Suneth Kariyawasam",
                    "user_href": "/profile/Suneth-Kariyawasam"
                },
                {
                    "user_id": "Mackendy Antitone",
                    "user_href": "/profile/Mackendy-Antitone"
                },
                {
                    "user_id": "Ariel Chels\u0103u",
                    "user_href": "/profile/Ariel-Chels\u0103u-1"
                },
                {
                    "user_id": "William Ritson",
                    "user_href": "/profile/William-Ritson"
                },
                {
                    "user_id": "Simon Chatzigiannis",
                    "user_href": "/profile/Simon-Chatzigiannis"
                },
                {
                    "user_id": "Bogdan Ianatiev",
                    "user_href": "/profile/Bogdan-Ianatiev"
                },
                {
                    "user_id": "Dean Carpenter",
                    "user_href": "/profile/Dean-Carpenter-1"
                },
                {
                    "user_id": "Ken Nickerson",
                    "user_href": "/profile/Ken-Nickerson"
                }
            ]
        }
    ]
}