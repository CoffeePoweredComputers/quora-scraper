{
    "title": "Would Google 'naturally' get slower over time? - Quora",
    "tags": [
        "Web Search",
        "Google Search",
        "Search Engines"
    ],
    "response": [
        {
            "author_info": {
                "name": "Jason S. Mow",
                "href": "/profile/Jason-S.-Mow"
            },
            "answer_text": "They have multiple systems that index different parts of the web. For example, one indexer focuses primarily on frequently updated sites (like news websites), and others focus on the \"rest of the web\". These rest of web indices don't need to be updated 100% of the time to be fresh, since these websites don't change incredibly frequently. Even if they do, the previous indexing was likely similar to the current state of the site save for minor updates and changes. The consistency requirements on these pages is not high since clicking the link still takes you to the real page that is up to date. Another thing not considered by the question is the decreasing cost of machines and Google's massive strides in building these machines for cheaply and running them in massively parallel clusters. ",
            "date": "Answered January 24, 2013",
            "views": "5",
            "upvotes": " View 1 Upvoter",
            "upvoters": [
                {
                    "user_id": "Mohamed El-Shinnawy",
                    "user_href": "/profile/Mohamed-El-Shinnawy"
                }
            ]
        },
        {
            "author_info": {
                "name": "Bruce R. Miller",
                "href": "/profile/Bruce-R-Miller"
            },
            "answer_text": "As fast as the web is growing, Google is growing faster. Indexing is done on a distributed basis, and Google is constantly adding new machines to participate in the work process.  Not only that, the machines are constantly being upgraded to the latest technology.  Additionally, Google is on the cutting edge of new types of hardware, which I can not speak about. Finally, the engineers are always designing new algorithms to maximize the amount of computation while minimizing the amount of electricity required.  Jason pointed out a good example of one of those algorithms: scanning frequently changing sites more often than other sites. It you look at historical latency, Google has gotten faster much more quickly than the web has grown. ",
            "date": "Answered January 24, 2013",
            "views": "957",
            "upvotes": " View 11 Upvoters",
            "upvoters": [
                {
                    "user_id": "Supratik Majumdar",
                    "user_href": "/profile/Supratik-Majumdar"
                },
                {
                    "user_id": "Nikita Dawda",
                    "user_href": "/profile/Nikita-Dawda"
                },
                {
                    "user_id": "Thomas Ng",
                    "user_href": "/profile/Thomas-Ng"
                },
                {
                    "user_id": "Allen Cheung",
                    "user_href": "/profile/Allen-Cheung"
                },
                {
                    "user_id": "Mohamed El-Shinnawy",
                    "user_href": "/profile/Mohamed-El-Shinnawy"
                },
                {
                    "user_id": "Komal Ramchandani",
                    "user_href": "/profile/Komal-Ramchandani"
                },
                {
                    "user_id": "Jeremy Hoffman",
                    "user_href": "/profile/Jeremy-Hoffman"
                },
                {
                    "user_id": "Mathangi Venkatesan",
                    "user_href": "/profile/Mathangi-Venkatesan"
                },
                {
                    "user_id": "Qasim Zeeshan",
                    "user_href": "/profile/Qasim-Zeeshan"
                },
                {
                    "user_id": "Abbas Halai",
                    "user_href": "/profile/Abbas-Halai"
                }
            ]
        }
    ]
}