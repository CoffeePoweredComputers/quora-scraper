{
    "title": "What are the distinct features of a server that differentiate it from a typical tower computer, in terms of parts and components? - Quora",
    "tags": [
        "Web Servers",
        "Servers (computers)"
    ],
    "response": [
        {
            "author_info": {
                "name": "Leonid S. Knyshov",
                "href": "/profile/Leonid-S.-Knyshov"
            },
            "answer_text": "There are small tower form factor servers, but they are rebadged desktops used by smallest businesses without a rack. The majority of mainstream servers go into racks. Mainstream servers consist of two styles: Rackmount serverBlade serverBoth of them mount in a standard 19\u2033 wide 4-post cabinet with standard 72 rack units (72U). Each rack unit is 1.5\u2033. It is possible to mount equipment into 2-post racks. Server chassis is described as 1U, 2U, 3U, 4U, 5U (a tower on its side), and beyond. Everything that goes into racks, including storage, power, networking, and other related gear, is measured in Us. Standard rackmount servers mount directly into the rack. Blade servers mount inside enclosures. Enclosures mount into the rack. Space is at premium for datacenter equipment. You usually want to use the thinnest equipment so you can mount more of it. There are 2U servers featuring more than 60 cores. Intel has Xeons with 28 cores. A key distinction is CPU cooling. Rack servers don\u2019t have CPU fans. They have chassis-wide fans. Air flows at very high volume from front to back across passive heat sinks. Servers are very loud. At full power, the fans of just one server can be as loud as a vacuum cleaner. Noise pollution is a lower priority for servers that live in racks in a dedicated room. Many servers feature IPMI, which permits troubleshooting of dead servers after the operating system no longer boots. Servers are designed for rapid service. I can completely replace every component in a server in less than 15 minutes if I am familiar with its chassis. First time service of a new chassis may take an hour. One particularly nasty design took me two hours. Every component in a server connects into a backplane. There are power backplanes, storage backplanes, and sometimes others. It is possible to use the PCI Express slots with so-called \u201criser\u201d cards, but most servers have nothing in their PCI Express slots. Functionally, there is nothing wrong with running a server as a desktop or a high end workstation. I do that often because servers depreciate to extremely low prices. I just quoted an example the other day of a 40-core server for $699. That is sheer lunacy. Each of those CPUs used to cost more than $4500!  ",
            "date": "Answered January 17, 2017",
            "views": "294",
            "upvotes": " View 2 Upvoters",
            "upvoters": [
                {
                    "user_id": "Bob Hannent",
                    "user_href": "/profile/Bob-Hannent"
                },
                {
                    "user_id": "Jason Yan",
                    "user_href": "/profile/Jason-Yan-22"
                }
            ]
        },
        {
            "author_info": {
                "name": "Jason Yan",
                "href": "/profile/Jason-Yan-22"
            },
            "answer_text": "Usually in rackmount RU size to fit inside a standard server cabinet. Redundant hotswap dual power supply. One fails, it keeps running on second. Front hotswap SATA trays for drives. Run some variant of RAID1 or 5. CPUs only have passive heatsinks, and high-CFM 40mm fans blow over the entire internals to cool everything. Fans are stacked in doubles for redundancy. Loud as heck. Server motherboard that use server CPUs such as AMD Opteron or Intel Xeon using ECC ram. Many people might think ECC is not necessary, but I\u2019ve seen ECC real-time error corrections in /var/log/messages more often than you think. Necessary if you have mission-critical data or files. Typically have multiple NICs. Sometimes you want to run etherchannel (multiplex 4 NICs together to get 4000Gbps throughput). Sometimes you need to be on more than one internal network. ",
            "date": "Answered January 17, 2017",
            "views": "243",
            "upvotes": " View 1 Upvoter",
            "upvoters": [
                {
                    "user_id": "Leonid S. Knyshov",
                    "user_href": "/profile/Leonid-S.-Knyshov"
                }
            ]
        }
    ]
}