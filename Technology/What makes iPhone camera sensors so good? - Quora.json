{
    "title": "What makes iPhone camera sensors so good? - Quora",
    "tags": [
        "Smartphone Cameras",
        "Sensors",
        "Digital Cameras",
        "Cameras",
        "iPhones (product)",
        "Apple Products and Services"
    ],
    "response": [
        {
            "author_info": {
                "name": "Jona Arkenson",
                "href": "/profile/Jona-Arkenson"
            },
            "answer_text": "Nothing. It's NOT the camera sensors, Apple uses the same exact Sony IMX sensors as any other flagship smartphone.. the two known smartphone camera sensor makers are Sony and Samsung.. but it appears that Apple uses Sony's cameras. What makes Apple's cameras so good is the CAMERA SOFTWARE, not the hardware necessarily. Apple automatically enhances the photo before taking it, this is called \"post-processing\" and since it has a very powerful CPU, the photo gets enhanced immediately (most of it is simply HDR boost and auto color correction and enhancement). Apple likely hired professional photographers to help adjust and tweak the camera software to make it look very beautiful.. so in a way, it's basically automatic photo shopping. This is the same reason Google's Pixel phone has one of the best looking camera photos\u2026 Google has done an excellent job tweaking the software to automatically enhance the photo to look very beautiful.. again, it's mostly HDR and color enhancement.. there's no special hardware or camera sensors\u2026 it's just a Sony IMX camera sensor. If you download the Snapseed photo editing app and then boost the ambient, increase a bit of HDR and tone, and maybe a bit of sharpness, you will get the same exact results. Professional photographers could see that the iPhone uses auto photo enhancement, some people do not like that and prefer the photo to be natural looking. A professional DSLR camera does NOT automatically enhance the photo as the camera does not have the software and CPU to do that, so a professional photographer has to manually photo edit the photos using something like Adobe Photoshop\u2026 but with a smartphone, it's all done automatically and you don't need to enhance it. Do you remember the iPhone X? The camera used too much HDR and the photo ended up looking very red and ugly.. and they also use a fake bokeh blur effect which looks very artificial\u2026 so you could see what's being done here with the iPhone camera, it's camera software tweaking. Unfortunately, iPhones do not have a \"manual mode\" so you will always have to use the auto mode. So the camera sensor is just a Sony IMX sensor which is used among all other flagships. Samsung has recently been making their own ISOCELL sensors, these sensors could have very high megapixels.. like 108MP camera on the Galaxy S20 Ultra and the 64MP camera on the LG V60. ",
            "date": "Answered October 10, 2020",
            "views": "65",
            "upvotes": "0"
        },
        {
            "author_info": {
                "name": "Dave Haynie",
                "href": "/profile/Dave-Haynie"
            },
            "answer_text": "It's primarily your expectations, your experience with prior cameras or phones, and your photographic expertise that determine whether an iPhone sensor is \u201cso good\" or perhaps, not so good.  The primary image sensor in current iPhone is a 1/2.55\" chip, a bit smaller than the smallest sensor shown in this chart. It's nearly identical to the sensors used in the Google Pixel series, and very similar to mainstream image sensors going back to about 2014. The chip used in professional cameras will collect 15\u2013100x more light, depending on the particular sensor size and lens aperture. That's the key\u2026all else being equal, collecting more light yields a better image. A larger sensor size allows for more pixel and color resolution, yielding a sharper image with better color and better editability. Having that, it makes sense to spend more than $3.00 or so on the camera lens, and do use real glass\u2014exotic glass in better lenses\u2014rather than the molded plastic used in phone lenses. It's Not the Camera Hardware So why are you happy with your iPhone results? Several reasons. Fundamentally, smartphones today are the bottom rung of consumer cameras, and despite the tiny sensors, the best all around low end consumer cameras anyone has come up with. Why? It's not the hardware. More specifically, it's not the camera hardware. Some recent phones have larger, more advanced sensors that in the iPhone, but with that comes additional cost and additional camera module thickness. A couple decades of smartphone experience has shown that most users value a thin phone. Improving the sensor by much means a larger chip, demanding a longer focal length lens to deliver the same size image, and thus, thinker phones. If you think about it, while most folks understand their phone isn't a camera, they don't quite acknowledge that it's not a phone, either. Rather, it's a pocket-sized personal computer. And that is the thing that Apple, Google, and others are using to make better images than you had in phones of the past. It Is the Computer! So the computer in your phone is very powerful compared to even professional cameras. When I shoot a photo with a professional digital camera, I press the shutter button. This causes the mirror to flip away (if it's a DSLR), the shutter to open, the photodiodes on the image sensor to collect light, the shutter to close. Next, an image signal processor reads the image from the sensor. Quite often, it does little more than put that data into a file format and write it to memory. In fact, much of a pro camera's CPU is used for advanced autofocus features. When you open the camera app on an iPhone 11, it starts shooting photos\u2026it's always shooting photos. When you press the shutter icon, in good light, it actually stops. It will take the last eight photos stored in memory, in pairs of one correct exposure and one underexposure. Those exposures are determined by an artificial intelligence agent, one of many. Another AI will go through the four sets of photos and pick the best, based on the AI's criteria. Those two shots are merged into one, a process generically known as computational photography. The normal and underexposed images together represent twice the light collection of a single exposure, and as well an increase in dynamic range (the ratio of light to dark in an image). The result is a single image better than any single image one could shoot with a single shot on that sensor. In lower light, the iPhone software will use up to eight shots in a single computational function. Apple calls this deep fusion. So that is now using eight times as much light as a single shot. Will that be as good as a good single shot from an 8x-larger sensor? No\u2026but it's pretty close. In very low light, the iPhone 11 will keep the eight shots but take one additional long exposure. That risks blurring, but get less noise and better color than the previous eight shots. When these shots are computationally fused into one, the long shot is processed by an AI that recolors the image. This is why you get a decent, usable shot in low light, but you may see odd colors, you may not see the shadowy areas of the original scene, etc. In short, the AIs are making all of your technical choices, but a few that are fundamentally creative ones as well. For users who have never learned much about photography, this basically gives you at least a taste of the photographic superpowers of a seasoned photographer. And because this \u201cskill\" level has been rapidly rising as Apple, Google, Samsung, Huawei, Xiaomi, etc. compete each year for the top phone camera accolades, your shot gets better every time you upgrade, even if you have learned nothing about photography. Additional Notes So, okay, I left out a few details. For one, phone sensors have changed a bit since 2014. Apple was using even tinier 1/3.2\u2033 and 1/3\u2033 chips, one reason their game was so strong when they went to the same 1/2.55\"-1/2.60\" size that had been around nearly forever. There were innovations, just not in resolution or much in the way of image quality. Autofocus was improved, and sensor readout speed greatly increased\u2014that fast sensor is what lets Apple just keep shooting images without you even noticing. One reason most phone shooters think their 2019\u20132020 images are so good is that they are improved from previous years' phones. But if you're a phone shooter, that's probably your main point of comparison. You see improvements, but you're not comparing your shots to pro cameras, high end point and shoot, or even to contemporary images from other phone brands. And finally, phone photographers are far more likely to do it all online. A full 4K display is about 8 megapixels at 8-bit color\u2014the difference between your phone and a pro's shot is diminished. Especially if you're viewing on the phone. Social media sites crunch down your uploads, losing quality, and Instagram chops resolution, too, down to just over 1 megapixel. So comparing your phone shots from phone or desktop to online images, you always have a visual advantage. ",
            "date": "Updated October 8, 2020",
            "views": "77",
            "upvotes": " View 93 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Sudarshan Konge",
                    "user_href": "/profile/Sudarshan-Konge"
                },
                {
                    "user_id": "Douglas Ivey",
                    "user_href": "/profile/Douglas-Ivey-1"
                },
                {
                    "user_id": "Il Kiou",
                    "user_href": "/profile/Il-Kiou"
                },
                {
                    "user_id": "Anindya Permata Widhi",
                    "user_href": "/profile/Anindya-Permata-Widhi"
                },
                {
                    "user_id": "Joe Krepps",
                    "user_href": "/profile/Joe-Krepps-1"
                },
                {
                    "user_id": "Anthony May",
                    "user_href": "/profile/Anthony-May-2"
                },
                {
                    "user_id": "Esat Uzundemir",
                    "user_href": "/profile/Esat-Uzundemir"
                },
                {
                    "user_id": "Gabriel Yu",
                    "user_href": "/profile/Gabriel-Yu-13"
                },
                {
                    "user_id": "Maciej Gizinski",
                    "user_href": "/profile/Maciej-Gizinski"
                },
                {
                    "user_id": "Alain Van Ruyskensvelde",
                    "user_href": "/profile/Alain-Van-Ruyskensvelde"
                }
            ]
        }
    ]
}