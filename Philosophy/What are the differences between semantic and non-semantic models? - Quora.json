{
    "title": "What are the differences between semantic and non-semantic models? - Quora",
    "tags": [
        "Semantic Search",
        "Semantic Web",
        "Semantics"
    ],
    "response": [
        {
            "author_info": {
                "name": "Natalia D\u00edaz Rodr\u00edguez",
                "href": "/profile/Natalia-D\u00edaz-Rodr\u00edguez"
            },
            "answer_text": "Semantic model are normally human-readable and easy to understand or grasp their meaning by humans since they are close to natural language. On the contrary, non-semantic models are normally those associated with black-box approaches, where we may not know the variables interactions a priory, dependencies or correlations among them, and we are to learn the functions underlying their representation. We could argue that semantic models inherently have higher bias than variance, since they impose some known structure, rules or properties a priori for the data you are trying to model. Semantic models are useful when sufficient expert knowledge in a given field, or common sense knowledge about the phenomenon to model are available, because there are many rule-based, ontologies and other knowledge representation frameworks such as probabilistic programming that allow easy translation of common sense/expert knowledge into machine readable formats for automatic inference. The utility of each rule can be found in semantic models and provides interpretability and fine tuning to the model, while in non-semantic models it is more hyperparameter optimization what is needed, in a less intuitive manner, to adjust the model. Both of these stages are nearly, if not the most, time consuming sub-tasks in machine learning that we are not yet able to delegate in machines to decide on. ",
            "date": "Answered August 21, 2016",
            "views": "22",
            "upvotes": " View 10 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Sz\u0151ll\u0151si Anna",
                    "user_href": "/profile/Sz\u0151ll\u0151si-Anna"
                },
                {
                    "user_id": "Asmita Suman",
                    "user_href": "/profile/Asmita-Suman-3"
                },
                {
                    "user_id": "Sergio Silva",
                    "user_href": "/profile/Sergio-Silva-32"
                },
                {
                    "user_id": "Sidney Kan",
                    "user_href": "/profile/Sidney-Kan"
                },
                {
                    "user_id": "Amit Jadhav",
                    "user_href": "/profile/\u0905\u092e\u093f\u0924-\u091c\u093e\u0927\u0935-Amit-Jadhav"
                },
                {
                    "user_id": "Quora User",
                    "user_href": "/profile/Miguel-\u00c1ngel-MF"
                },
                {
                    "user_id": "Paul King",
                    "user_href": "/profile/Paul-King-2"
                },
                {
                    "user_id": "Antonio Penta",
                    "user_href": "/profile/Antonio-Penta-1"
                },
                {
                    "user_id": "Pavel Kucherbaev",
                    "user_href": "/profile/Pavel-Kucherbaev"
                },
                {
                    "user_id": "Syrus Akbary Nieto",
                    "user_href": "/profile/Syrus-Akbary-Nieto"
                }
            ]
        },
        {
            "author_info": {
                "name": "Alan Morrison",
                "href": "/profile/Alan-Morrison"
            },
            "answer_text": "I thought Pedro Domingos  in The Master Algorithm (The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World: Pedro Domingos: 9780465065707: Amazon.com: Books) helped answer this question best by categorizing the five tribes of machine learning. In Domingos' categorization, the Symbolists are the ones who  develop and discover semantic models.  Mike Bergman in his February 2016 post \"Pulse: The Biggest of the Big Pictures on Machine Learning\" summarized Domingos' categories well:  Symbolists \u2014 based in logic, this approach attempts to model the composition of knowledge by inverting the deductive processConnectionists \u2014 also known as neural networks or deep learning, this mindset is grounded most in trying to mimic how the brain actually worksEvolutionists \u2014 the biological evolution of life of mixing genes through reproduction as altered by mutations and cross-overs guides these genetic algorithmsBayesians \u2014 since the world is uncertain, likely outcomes are guided by statistical probabilities, which also change as new evidence is constantly brought to bearAnalogizers \u2014 this tribe attempts to reason by analogy by looking for similarities to examples or closely related factors.According to Webster,  semantics is \"the meanings of words and phrases in a particular context.\" (See Definition of SEMANTICS.) To build these contexts, Symbolists  create, articulate and discover contextual models that can be used to accurately identify people, places and things and the dynamic relationships between them. Relationships are what define and articulate the context between entities. Applied well, relationship logic (set theory) ensures the accuracy of a given semantic model. Creating webs of context and a contextual web is the goal of the semantic web standards effort. Google's Knowledge Graph and Freebase in particular, which Google bought and expanded to create Knowledge Graph, were examples of such a web. The successor Knowledge Vault provides a probabilistic means of scaling such a web, presumably borrowing techniques from the Bayesian tribe to be used in conjunction with the techniques the Symbolist Freebase developers had been using.  See https://www.cs.ubc.ca/~murphyk/Papers/kv-kdd14.pdf  Of course,the Symbolists might be appalled at the use of probabilistic methods to define the differences between one person, place or thing and another and how they interrelate and interact because such methods are inherently less than accurate--they're a form of guesswork that tolerates a degree of uncertainty. The counterargument to the Symbolists might be, while fingerprints and DNA signatures provide a high degree of certainty, not everything can be known with certainty. And we do need to scale knowledge bases somehow. ",
            "date": "Updated March 29, 2016",
            "views": "2",
            "upvotes": " View 3 Upvoters ",
            "upvoters": [
                {
                    "user_id": "Sidney Kan",
                    "user_href": "/profile/Sidney-Kan"
                },
                {
                    "user_id": "Abhishek Kunnath",
                    "user_href": "/profile/Abhishek-Kunnath"
                },
                {
                    "user_id": "Ivan Braun",
                    "user_href": "/profile/Ivan-Braun-1"
                }
            ]
        }
    ]
}