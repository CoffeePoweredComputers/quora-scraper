{
    "title": "What is a good exam? - Quora",
    "tags": [
        "Regional Transport Office",
        "Educational Technology",
        "Government of India",
        "University Exams and Tests",
        "Schools",
        "Education",
        "College Advice"
    ],
    "response": [
        {
            "author_info": {
                "name": "Igor Markov",
                "href": "/profile/Igor-Markov"
            },
            "answer_text": "Exams serve multiple purposes, must be fair, and must be feasible to administer.An exam that looks/feels good to students is not necessarily a good exam -- most students prefer exams that are easy to take (simple questions that closely resemble practice problems). Professors are usually biased toward exams that are easy to construct (a small number of deep questions) or easy to grade (a large number of shallow multiple-choice questions).  You can only confirm a good exam after hearing students' complaints (or lack thereof) and seeing grade statistics. An exam that is good for one group of students may be bad for another group of students.  An exam must be \"clean\" --- no obvious flaws or hidden ambiguities --- otherwise, exam-takers won't take it seriously and/or the scores may be random.An exam must cover a well-defined scope and certain skills. It is important to avoid unintentional biases to particular topics and problem types. The most common bias is towards the knowledge or understanding of the material and away from the skills needed to apply the material, such as problem-solving skills (sometimes derogatory referred to as exam-taking skills).An exam should evaluate students' knowledge and abilities, rather than familiarity with a small set of practice problems or patterns (otherwise, the grades may depend too much on which student missed which lecture).An exam should not be sensitive on factors irrelevant to the scope/skills (such as the anecdotes told by the instructor in class, cultural references, the material tested for by courses other than prerequisites).Assuming that exam-takers have different levels of proficiency, the exam should differentiate between them. Mathematically speaking, the entropy in exam scores (on a 100-point scale, with 100 or 50 bins) should be at least 3.5 bits, but hopefully >4 bits. I am surprised how few professors compute and report entropy (or even know what it is).The exam should be resistant to cheating and gaming. In particular, reusing the same problems every year verbatim is not a good idea. Take-home exams have obvious risks, and you can't use multiple-choice questions on those. Allowing the use of electronics during an exam would be asking for trouble.It is desirable to make the exam a learning experience.Given that exams usually last between 1 and 2 hours, it is difficult to cover the entire scope without at least some multiple-choice questions. But for MC questions to work well, you need at least 10 of them (to have the law of large numbers on your side). You also need some essay-type of questions, but those are hard to grade. I also use a third type --- short-calculation --- where the answers (say, large integers) are easy to grade, but can't be guessed. When preparing exams, I collect proposed exam questions from TAs, add a number of my own, and then send an exam draft with more questions than needed to the TAs. The TAs take the exam with a clock ticking and mark several questions as bad. The bad questions are removed, and the good questions are tuned to avoid ambiguities. If someone thinks they can prepare a high-quality exam alone from scratch, they are mistaken (your best bet is to dress/tweak known-good questions to look different on the surface). A lot can be said about processing grade statistics. For multiple-choice questions, our evaluations office produces a special statistics saying how answers on each question correlated with the overall performance. Once in a while, you'll see a question that does not correlate to the overall performance --- these deserve some scrutiny (usually, they are very easy or very hard questions). It also helps to see if the most popular answer to an MC question is correct, and if the correct answer was picked by >50% students.   ",
            "date": "Updated December 25, 2013",
            "views": "193",
            "upvotes": " View 112 Upvoters",
            "upvoters": [
                {
                    "user_id": "Meghna Ranjan",
                    "user_href": "/profile/Meghna-Ranjan-5"
                },
                {
                    "user_id": "Sanjay Sethuraman",
                    "user_href": "/profile/Sanjay-Sethuraman-1"
                },
                {
                    "user_id": "David C.L. Lee",
                    "user_href": "/profile/David-C-L-Lee"
                },
                {
                    "user_id": "Sandy Storey",
                    "user_href": "/profile/Sandy-Storey-1"
                },
                {
                    "user_id": "Dima Bookshpan",
                    "user_href": "/profile/Dima-Bookshpan"
                },
                {
                    "user_id": "Lya Nurliana",
                    "user_href": "/profile/Lya-Nurliana"
                },
                {
                    "user_id": "S\u00e9rgio Luis Clem",
                    "user_href": "/profile/S\u00e9rgio-Luis-Clem"
                },
                {
                    "user_id": "Claudia Calvi",
                    "user_href": "/profile/Claudia-Calvi"
                },
                {
                    "user_id": "Aditya Kumar Gupta",
                    "user_href": "/profile/Aditya-Kumar-Gupta"
                },
                {
                    "user_id": "Sam Wiegand",
                    "user_href": "/profile/Sam-Wiegand-1"
                }
            ]
        },
        {
            "author_info": {
                "name": "Jenny Grant Rankin",
                "href": "/profile/Jenny-Grant-Rankin"
            },
            "answer_text": "Below is a collection of excerpts from the \u201cAssessment Design\u201d manual I wrote for Illuminate Education\u2019s help system, which reflects research-based best practices for exam questions and each exam as a whole: EACH QUESTION In addition to others\u2019 answers about the exam as a whole, there are best practices for each exam question.  ContentEach question should be designed to measure something specific. If a question measures too many things, you won\u2019t know where a student is struggling when the question is missed. The question should also match the content standard (being assessed) in terms of content. The most widely-used content standards in K-12 education are currently the CCSS (Common Core State Standards Initiative). For example, if the content standard requires an American fable, does the question relates to an American fable? RigorThe question should match the standard in terms of rigor. For example, if the standard requires students to evaluate arguments contributing to the development of the Constitution, the question should require students to evaluate. Consider the Bloom's Taxonomy level of the standard being assessed. VocabularyThe vocabulary should be appropriate for the standard level being assessed. You should not turn the assessment into a vocabulary test. In other words, you should not include high academic vocabulary that is not appropriate for the standard being assessed - otherwise your results could imply students are struggling with a standard when in fact they were struggling with the language. However (and this is a huge \"however\"), remember that the vocabulary should be appropriate for the standard, and you should use terms the standard and/or real life will require in conjunction with the concept (i.e., don't refer to subtraction as \"take aways\" on your test because that is the way you refer to subtraction when you teach - use terminology the student will encounter and should understand). If you are concerned about English Learners and Special Education students taking a test, remember that you may opt (in a uniform way for all teachers administering the test) to emulate state assessment practices of allowing the use of definition glossaries while taking the test for some students. ClarityThe question should be clear in what it\u2019s asking and as succinct as possible. Also, there should be no misleading distractors (e.g., don't try to \"trick\" students). Freedom from BiasThe question should be free of bias, meaning that students of all backgrounds should have an equal chance of getting it right. For example, would socioeconomically disadvantaged students be at a disadvantage due to lack of familiarity with something (e.g., a ski lift analogy used in a slope calculation question)? ...or might non-Hispanic students be confused by the term Quincea\u00f1era? Meaningful DistractorsWrong answer options should be selected carefully based on the likelihood of their selection and the information they will offer educators after the test. Consider this example:14 x 6 =A. 20B. 30C. 64D. 84 A will let me know which students added rather than multiplied (perhaps they need help spotting details, or perhaps they don't know how to multiply).B will let me know which students multiplied 6 by each number in 14 and then added the 2 together.C will let me know which students forgot to carry the 2 when multiplying.D is the correct answer.Logical Distractor OrderWhen the answers are numeric, list them from low to high. Positive WordingQuestions should be positive in terms of how they are worded and/or implications they make. For example, avoid using the terms not, none of the following, except, etc., and if you must use such terms, place them in all capital letters (e.g., \"NOT\"). IndependenceWhile multiple choice questions may share the same stimulus material, all questions should function independently from one another. For example, answering a question correctly should not rely on having answered a previous question correctly, nor should it rely on (or be helped by) information revealed in another question (within the stem or answer options).Consider all of the following in the way of format: ContextConsider the best context with which to ask the question. Keep in mind that not all questions/items or assessments must involve multiple choice, but keep efficiency and resources in mind if a different format is absolutely necessary. In other words, do not abandon multiple choice simply because you do not \"like\" multiple choice; rather, abandon it if it is truly not the best way to assess a standard (e.g., a \"Listening and Speaking\" English-Language Arts strand standard, staff members have a solid understanding of authentic assessments, etc.). # of Answer OptionsOn a multiple choice test, 2 or 4-5 answer options is the desirable number. Avoid 3 (easier to answer than 4) unless you are seeking to simplify the test (e.g., as the California Modified Assessment is like a simplified version of the California Standards Test for Special Education students), since it increases the likelihood of students merely guessing correctly. It is best if all questions on the test contain the same number of answer options. If this is not possible, try to group questions with the same number of answer options together. LengthAnswer options should be approximately the same length. Instructions directed at the student (e.g., \"Read the passage below and answer the questions that follow\") or at the test administrator (e.g., common for lower-elementary levels where the teacher reads questions to the class) should be as brief and direct as possible. LOOK You might opt to mirror the look of standardized assessments in terms of how questions are numbered, how answer options are itemized, how many columns are used, how much white space is on a page, etc. If this assessment is one in a series, they should all maintain a cohesive look. Remember that while there are numerous advantages to multiple choice tests (e.g., they make a good start to an assessment program, especially if your colleagues are resistant, they keep scoring objective, they facilitate instant feedback for students/parents/educators, they save educators time, they are cost effective, etc.), they also have limitations. Your assessment might thus feature a combination of assessment types. TAKE THE TEST That's right. Take the exam, just as a student would - preferably a week or so after you wrote or selected questions for it (or have others take the test who weren't involved in the questions writing/selection). Note which questions and test sections are easiest, which are hardest, which could render unnecessary confusion for students, etc. Even though careful thought already went into each question, you can still catch problems at this stage. Also, you want to get a feel for overall rigor and balance. Do you already spot changes that need to be made? BALANCE & THE BIG PICTURE This section applies to the test as a whole and its sections (not individual questions): Breadth/Scope of StandardStandards often require multiple things of students. Do the questions on your test appropriately cover the breadth of the standard, or are they limited to only one of its aspects? Even if the questions are well-crafted, you might need to replace some questions with others to thoroughly assess a standard. RigorDoes the collection of questions assessing a standard match the rigor the standard requires? For example, if the standard requires students to evaluate arguments contributing to the development of the Constitution, has the test successfully required students to evaluate. Consider the Bloom's Taxonomy level of the standard being assessed. # of Questions per StandardConsider the assessment as a whole, the pacing guide, and the assessment series as a whole. For example, does the pacing guide note this assessment should cover 5 standards, whereas your 20-question tests contains 11 questions on a relatively simple standard, leaving just 9 questions to assess the remaining 4 standards? That would be a problem you'd want to remedy. A bare minimum of 3-4 questions is typically needed to accurately assess mastery of a standard, though this number can vary based on standard scope and complexity. If you are mirroring blueprints or question allotments determined ahead of time, be sure to compare the assessment to these. IndependenceWhile multiple choice questions may share the same stimulus material, all questions should function independently from one another. For example, answering a question correctly should not rely on having answered a previous question correctly, nor should it rely on (or be helped by) information revealed in another question (within the stem or answer options). FORMAT Consider all of the following in the way of format: InstructionsIf there are pre-test instructions for students and teachers, are they as clear and brief as possible? Stimulus MaterialsIf stimulus material(s) are used to answer questions (e.g., graph, table, passage, map, picture, diagram, etc.), are the images of good quality, clear, etc.? Is their connection to the question(s) clear (i.e., will students know they have to use them to answer related questions)? ",
            "date": "Answered September 8, 2014",
            "views": "708",
            "upvotes": "0"
        }
    ]
}